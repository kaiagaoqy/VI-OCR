# Detection Order åŠŸèƒ½æ›´æ–°

## ğŸ“‹ æ›´æ–°è¯´æ˜

åœ¨ `totaltext_eval_FINAL.py` ä¸­æ–°å¢äº†**Detection Orderï¼ˆæ£€æµ‹é¡ºåºï¼‰**åŠŸèƒ½ï¼Œç”¨äºè®°å½•æ¯ä¸ªå•è¯åœ¨æ¯å¼ å›¾ä¸­è¢«humanæ£€æµ‹å‡ºæ¥çš„é¡ºåºã€‚

## ğŸ¯ åŠŸèƒ½æè¿°

### ä»€ä¹ˆæ˜¯Detection Orderï¼Ÿ

Detection Order æ˜¯æŒ‡æ¯ä¸ªå•è¯åœ¨è¾“å…¥JSONæ–‡ä»¶ä¸­å‡ºç°çš„é¡ºåºï¼ˆä»0å¼€å§‹è®¡æ•°ï¼‰ã€‚è¿™ä¸ªé¡ºåºåæ˜ äº†ï¼š
- Humanæ ‡æ³¨å‘˜æ ‡è®°å•è¯çš„é¡ºåº
- å¯èƒ½åæ˜ è§†è§‰æ‰«æçš„é¡ºåº
- å¯ä»¥ç”¨äºåˆ†ææ˜¯å¦å­˜åœ¨ä½ç½®åè§ï¼ˆposition biasï¼‰

### ç¤ºä¾‹

å‡è®¾ä¸€å¼ å›¾ç‰‡çš„JSONæ•°æ®å¦‚ä¸‹ï¼š

```json
{
  "image_id": 123,
  "polys": [
    [[10, 20], [30, 20], [30, 40], [10, 40]],  // Detection Order = 0
    [[50, 60], [80, 60], [80, 90], [50, 90]],  // Detection Order = 1
    [[100, 30], [130, 30], [130, 50], [100, 50]] // Detection Order = 2
  ],
  "rec_texts": ["STORE", "OPEN", "NOW"]
}
```

é‚£ä¹ˆï¼š
- "STORE" çš„ Detection Order = 0ï¼ˆç¬¬ä¸€ä¸ªè¢«æ£€æµ‹ï¼‰
- "OPEN" çš„ Detection Order = 1ï¼ˆç¬¬äºŒä¸ªè¢«æ£€æµ‹ï¼‰
- "NOW" çš„ Detection Order = 2ï¼ˆç¬¬ä¸‰ä¸ªè¢«æ£€æµ‹ï¼‰

## ğŸ“Š Excelè¾“å‡ºå˜åŒ–

### Sheet 1: Word Pairs

æ–°å¢äº† **"Detection Order"** åˆ—ï¼Œä½äºç¬¬äºŒåˆ—ï¼ˆImage Nameä¹‹åï¼‰ï¼š

| åˆ—å | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| Image Name | å›¾åƒæ–‡ä»¶è·¯å¾„ | `16/img_001.jpg` |
| **Detection Order** | **æ£€æµ‹é¡ºåºï¼ˆä»0å¼€å§‹ï¼‰** | **0, 1, 2, ...** |
| GT Text | Ground Truthæ–‡æœ¬ | `STORE` |
| Pred Text | é¢„æµ‹æ–‡æœ¬ | `STORE` |
| IoU | Polygon IoU | `0.8523` |
| Edit Distance | ç¼–è¾‘è·ç¦» | `0` |
| Word Match | æ˜¯å¦å®Œå…¨åŒ¹é… | `Yes` |
| Char F1 | å­—ç¬¦F1 | `1.0000` |

### ç¤ºä¾‹æ•°æ®

```
Image Name      Detection Order   GT Text   Pred Text   IoU     Edit Dist   Word Match   Char F1
16/img_001.jpg  0                 STORE     STORE       0.8523  0           Yes          1.0000
16/img_001.jpg  1                 OPEN      OPEN        0.7891  0           Yes          1.0000
16/img_001.jpg  2                 NOW       NOW         0.8234  0           Yes          1.0000
16/img_002.jpg  0                 SALE      SALE        0.7654  0           Yes          1.0000
16/img_002.jpg  1                 TODAY     TODAY       0.8123  0           Yes          1.0000
```

## ğŸ” åº”ç”¨åœºæ™¯

### 1. ä½ç½®åè§åˆ†æ

æ£€æŸ¥è¯†åˆ«å‡†ç¡®ç‡æ˜¯å¦ä¸æ£€æµ‹é¡ºåºç›¸å…³ï¼š

```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–Excel
df = pd.read_excel('results/excel_outputs/Sub002_evaluation.xlsx', 
                   sheet_name='Word Pairs')

# æŒ‰Detection Orderåˆ†ç»„ç»Ÿè®¡å‡†ç¡®ç‡
accuracy_by_order = df.groupby('Detection Order')['Word Match'].apply(
    lambda x: (x == 'Yes').sum() / len(x)
)

# å¯è§†åŒ–
plt.figure(figsize=(12, 6))
plt.plot(accuracy_by_order.index, accuracy_by_order.values, marker='o')
plt.xlabel('Detection Order')
plt.ylabel('Accuracy')
plt.title('Recognition Accuracy vs Detection Order')
plt.grid(True, alpha=0.3)
plt.show()

print("Accuracy by Detection Order:")
print(accuracy_by_order)
```

### 2. æ—©æœŸ vs æ™šæœŸæ£€æµ‹å¯¹æ¯”

```python
# å°†æ£€æµ‹é¡ºåºåˆ†ä¸ºæ—©æœŸå’Œæ™šæœŸ
df['detection_phase'] = df['Detection Order'].apply(
    lambda x: 'Early' if x < 5 else 'Late'
)

# å¯¹æ¯”å‡†ç¡®ç‡
phase_accuracy = df.groupby('detection_phase')['Word Match'].apply(
    lambda x: (x == 'Yes').sum() / len(x)
)

print("Early detections accuracy:", phase_accuracy['Early'])
print("Late detections accuracy:", phase_accuracy['Late'])
```

### 3. ç–²åŠ³æ•ˆåº”åˆ†æ

æ£€æŸ¥æ˜¯å¦éšç€æ£€æµ‹é¡ºåºå¢åŠ ï¼Œå‡†ç¡®ç‡ä¸‹é™ï¼ˆå¯èƒ½è¡¨ç¤ºæ ‡æ³¨å‘˜ç–²åŠ³ï¼‰ï¼š

```python
from scipy.stats import spearmanr

# å°†Word Matchè½¬æ¢ä¸ºæ•°å€¼
df['match_numeric'] = (df['Word Match'] == 'Yes').astype(int)

# è®¡ç®—ç›¸å…³æ€§
correlation, p_value = spearmanr(df['Detection Order'], df['match_numeric'])

print(f"Spearman correlation: {correlation:.3f}")
print(f"P-value: {p_value:.3f}")

if correlation < -0.1 and p_value < 0.05:
    print("âš ï¸ å‘ç°æ˜¾è‘—çš„è´Ÿç›¸å…³ï¼šæ£€æµ‹é¡ºåºè¶Šåï¼Œå‡†ç¡®ç‡è¶Šä½")
elif correlation > 0.1 and p_value < 0.05:
    print("âœ… å‘ç°æ˜¾è‘—çš„æ­£ç›¸å…³ï¼šæ£€æµ‹é¡ºåºè¶Šåï¼Œå‡†ç¡®ç‡è¶Šé«˜ï¼ˆå¯èƒ½å› ä¸ºå®¹æ˜“çš„è¯å…ˆè¢«æ£€æµ‹ï¼‰")
else:
    print("âœ“ æ— æ˜¾è‘—ç›¸å…³æ€§ï¼šæ£€æµ‹é¡ºåºä¸å½±å“å‡†ç¡®ç‡")
```

### 4. æ¯å¼ å›¾çš„æ£€æµ‹åˆ†å¸ƒ

```python
# æŸ¥çœ‹æ¯å¼ å›¾æœ‰å¤šå°‘ä¸ªæ£€æµ‹
detections_per_image = df.groupby('Image Name')['Detection Order'].agg(['count', 'max'])
detections_per_image.columns = ['Num_Detections', 'Max_Order']
detections_per_image['Max_Order'] += 1  # å› ä¸ºä»0å¼€å§‹

print("Images with most detections:")
print(detections_per_image.nlargest(10, 'Num_Detections'))

# ç»Ÿè®¡
print(f"\nAverage detections per image: {detections_per_image['Num_Detections'].mean():.1f}")
print(f"Max detections in one image: {detections_per_image['Num_Detections'].max()}")
```

### 5. ç‰¹å®šé¡ºåºçš„é”™è¯¯åˆ†æ

```python
# æ‰¾å‡ºåœ¨ç‰¹å®šé¡ºåºä½ç½®ä¸Šé”™è¯¯ç‡æœ€é«˜çš„æƒ…å†µ
df['is_error'] = (df['Word Match'] == 'No')

error_by_order = df.groupby('Detection Order').agg({
    'is_error': 'mean',
    'Image Name': 'count'
})
error_by_order.columns = ['Error_Rate', 'Count']

# åªçœ‹æœ‰è¶³å¤Ÿæ ·æœ¬çš„é¡ºåºä½ç½®
significant_orders = error_by_order[error_by_order['Count'] >= 10]
problematic_orders = significant_orders[significant_orders['Error_Rate'] > 0.3]

print("Problematic detection orders (error rate > 30%):")
print(problematic_orders)
```

## ğŸ”¬ ç ”ç©¶ä»·å€¼

### 1. æ ‡æ³¨è´¨é‡è¯„ä¼°
- **æ—©æœŸæ£€æµ‹**: é€šå¸¸æ˜¯æœ€æ˜¾è‘—çš„æ–‡æœ¬ï¼Œåº”è¯¥æœ‰é«˜å‡†ç¡®ç‡
- **æ™šæœŸæ£€æµ‹**: å¯èƒ½æ˜¯è¾ƒå°æˆ–éš¾è¯†åˆ«çš„æ–‡æœ¬ï¼Œå‡†ç¡®ç‡å¯èƒ½é™ä½
- **å¼‚å¸¸æ¨¡å¼**: å¦‚æœæ—©æœŸæ£€æµ‹å‡†ç¡®ç‡ä½ï¼Œå¯èƒ½è¡¨ç¤ºæ ‡æ³¨è´¨é‡é—®é¢˜

### 2. è§†è§‰æ‰«ææ¨¡å¼
- åˆ†æäººç±»è§†è§‰æ³¨æ„åŠ›çš„é¡ºåº
- æ£€æµ‹æ˜¯å¦å­˜åœ¨ä»å·¦åˆ°å³ã€ä»ä¸Šåˆ°ä¸‹çš„æ‰«ææ¨¡å¼
- è¯†åˆ«è§†è§‰æ˜¾è‘—æ€§ä¸æ£€æµ‹é¡ºåºçš„å…³ç³»

### 3. æ¨¡å‹æ€§èƒ½è¯„ä¼°
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¯¹æŸäº›ä½ç½®çš„æ–‡æœ¬æœ‰åè§
- è¯„ä¼°æ¨¡å‹æ˜¯å¦èƒ½ä¸€è‡´åœ°å¤„ç†æ‰€æœ‰ä½ç½®çš„æ–‡æœ¬
- å‘ç°å¯èƒ½çš„ä½ç½®ç›¸å…³æ€§èƒ½é—®é¢˜

### 4. æ•°æ®å¹³è¡¡
- ç¡®ä¿è®­ç»ƒ/æµ‹è¯•é›†åœ¨ä¸åŒæ£€æµ‹é¡ºåºä¸Šçš„åˆ†å¸ƒå¹³è¡¡
- é¿å…æ¨¡å‹å­¦ä¹ åˆ°ä¸é¡ºåºç›¸å…³çš„è™šå‡ç›¸å…³æ€§

## ğŸ“ˆ ç»Ÿè®¡åˆ†æç¤ºä¾‹

### å®Œæ•´åˆ†æè„šæœ¬

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import spearmanr, chi2_contingency

# è¯»å–æ‰€æœ‰subjectsçš„æ•°æ®
all_subjects = []
for subject_file in Path('results/excel_outputs').glob('Sub*_evaluation.xlsx'):
    df = pd.read_excel(subject_file, sheet_name='Word Pairs')
    df['Subject'] = subject_file.stem.split('_')[0]
    all_subjects.append(df)

df_all = pd.concat(all_subjects, ignore_index=True)

print("="*80)
print("DETECTION ORDER ANALYSIS")
print("="*80)

# 1. åŸºæœ¬ç»Ÿè®¡
print("\nã€åŸºæœ¬ç»Ÿè®¡ã€‘")
print(f"æ€»æ ·æœ¬æ•°: {len(df_all)}")
print(f"æœ€å¤§æ£€æµ‹é¡ºåº: {df_all['Detection Order'].max()}")
print(f"å¹³å‡æ¯å¼ å›¾æ£€æµ‹æ•°: {df_all.groupby(['Subject', 'Image Name']).size().mean():.1f}")

# 2. å‡†ç¡®ç‡ vs æ£€æµ‹é¡ºåº
print("\nã€å‡†ç¡®ç‡ vs æ£€æµ‹é¡ºåºã€‘")
df_all['is_correct'] = (df_all['Word Match'] == 'Yes')
accuracy_by_order = df_all.groupby('Detection Order')['is_correct'].mean()

# åªçœ‹æœ‰è¶³å¤Ÿæ ·æœ¬çš„é¡ºåº
valid_orders = df_all.groupby('Detection Order').size() >= 10
valid_accuracy = accuracy_by_order[valid_orders]

print(f"Detection Order 0-9çš„å‡†ç¡®ç‡:")
for i in range(min(10, len(valid_accuracy))):
    if i in valid_accuracy.index:
        print(f"  Order {i}: {valid_accuracy[i]:.1%} ({df_all[df_all['Detection Order']==i]['is_correct'].sum()}/{len(df_all[df_all['Detection Order']==i])})")

# 3. ç›¸å…³æ€§åˆ†æ
correlation, p_value = spearmanr(df_all['Detection Order'], df_all['is_correct'])
print(f"\nã€ç›¸å…³æ€§åˆ†æã€‘")
print(f"Spearmanç›¸å…³ç³»æ•°: {correlation:.3f}")
print(f"P-value: {p_value:.4f}")
if p_value < 0.05:
    print(f"{'âš ï¸ æ˜¾è‘—è´Ÿç›¸å…³' if correlation < 0 else 'âœ“ æ˜¾è‘—æ­£ç›¸å…³'}")
else:
    print("âœ“ æ— æ˜¾è‘—ç›¸å…³æ€§")

# 4. å¯è§†åŒ–
fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 4.1 å‡†ç¡®ç‡ vs æ£€æµ‹é¡ºåº
ax1 = axes[0, 0]
order_counts = df_all.groupby('Detection Order').size()
valid_orders = order_counts[order_counts >= 10].index
plot_data = accuracy_by_order[valid_orders]
ax1.plot(plot_data.index, plot_data.values, marker='o', linewidth=2)
ax1.set_xlabel('Detection Order')
ax1.set_ylabel('Accuracy')
ax1.set_title('Recognition Accuracy vs Detection Order')
ax1.grid(True, alpha=0.3)
ax1.axhline(y=df_all['is_correct'].mean(), color='r', linestyle='--', 
            label=f'Overall Avg: {df_all["is_correct"].mean():.1%}')
ax1.legend()

# 4.2 æ£€æµ‹é¡ºåºåˆ†å¸ƒ
ax2 = axes[0, 1]
order_dist = df_all['Detection Order'].value_counts().sort_index()
ax2.bar(order_dist.index[:20], order_dist.values[:20])
ax2.set_xlabel('Detection Order')
ax2.set_ylabel('Count')
ax2.set_title('Distribution of Detection Orders')
ax2.grid(True, alpha=0.3, axis='y')

# 4.3 Char F1 vs æ£€æµ‹é¡ºåº
ax3 = axes[1, 0]
char_f1_by_order = df_all.groupby('Detection Order')['Char F1'].mean()
plot_data = char_f1_by_order[valid_orders]
ax3.plot(plot_data.index, plot_data.values, marker='s', color='green', linewidth=2)
ax3.set_xlabel('Detection Order')
ax3.set_ylabel('Character F1')
ax3.set_title('Character F1 vs Detection Order')
ax3.grid(True, alpha=0.3)

# 4.4 æ—©æœŸ vs æ™šæœŸæ£€æµ‹å¯¹æ¯”
ax4 = axes[1, 1]
df_all['phase'] = pd.cut(df_all['Detection Order'], 
                         bins=[0, 3, 7, 100], 
                         labels=['Early (0-2)', 'Mid (3-6)', 'Late (7+)'])
phase_stats = df_all.groupby('phase')['is_correct'].agg(['mean', 'count'])
ax4.bar(range(len(phase_stats)), phase_stats['mean'], 
        color=['green', 'orange', 'red'], alpha=0.7)
ax4.set_xticks(range(len(phase_stats)))
ax4.set_xticklabels(phase_stats.index)
ax4.set_ylabel('Accuracy')
ax4.set_title('Accuracy by Detection Phase')
ax4.grid(True, alpha=0.3, axis='y')

# æ·»åŠ æ ·æœ¬æ•°æ ‡æ³¨
for i, (acc, count) in enumerate(zip(phase_stats['mean'], phase_stats['count'])):
    ax4.text(i, acc + 0.02, f'n={count}', ha='center', fontsize=10)

plt.tight_layout()
plt.savefig('results/detection_order_analysis.png', dpi=300, bbox_inches='tight')
print(f"\nâœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: results/detection_order_analysis.png")

plt.show()
```

## âš™ï¸ æŠ€æœ¯å®ç°

### ä»£ç å˜æ›´

#### 1. æ•°æ®ç»„ç»‡ï¼ˆLine 338-343ï¼‰
```python
text = rec_texts[i] if i < len(rec_texts) else ''
det_by_image[unique_key].append({
    'bbox': polygon_points,
    'text': text,
    'detection_order': i  # è®°å½•æ£€æµ‹é¡ºåº
})
```

#### 2. è¯„ä¼°å‡½æ•°ï¼ˆLine 157, 201ï¼‰
```python
detection_order = det.get('detection_order', -1)  # è·å–æ£€æµ‹é¡ºåº

matched_word_pairs.append({
    # ... å…¶ä»–å­—æ®µ ...
    'detection_order': detection_order  # æ·»åŠ åˆ°ç»“æœä¸­
})
```

#### 3. Excelè¾“å‡ºï¼ˆLine 465ï¼‰
```python
word_pairs_data.append({
    'Image Name': pair['image_name'],
    'Detection Order': pair['detection_order'],  # æ–°å¢åˆ—
    'GT Text': pair['gt'],
    # ... å…¶ä»–åˆ— ...
})
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **é¡ºåºä»0å¼€å§‹**: Detection Orderä»0å¼€å§‹è®¡æ•°ï¼Œç¬¬ä¸€ä¸ªæ£€æµ‹çš„å•è¯order=0
2. **JSONé¡ºåº**: Orderåæ˜ JSONæ–‡ä»¶ä¸­`polys`å’Œ`rec_texts`æ•°ç»„çš„é¡ºåº
3. **ç¼ºå¤±å€¼**: å¦‚æœæŸä¸ªdetectionæ²¡æœ‰orderä¿¡æ¯ï¼Œå°†æ˜¾ç¤ºä¸º-1
4. **Per-image**: æ¯å¼ å›¾çš„orderç‹¬ç«‹è®¡æ•°ï¼Œä¸åŒå›¾çš„order=0è¡¨ç¤ºå„è‡ªçš„ç¬¬ä¸€ä¸ªæ£€æµ‹

## âœ… éªŒè¯

### æ£€æŸ¥æ›´æ–°æ˜¯å¦æˆåŠŸ

è¿è¡Œè„šæœ¬åï¼Œæ‰“å¼€ä»»æ„Excelæ–‡ä»¶ï¼š

```bash
python untils/eval/totaltext_eval_FINAL.py
```

æ£€æŸ¥ `results/excel_outputs/Sub002_evaluation.xlsx` çš„ Sheet 1:
- âœ… åº”è¯¥çœ‹åˆ°ç¬¬äºŒåˆ—æ˜¯ "Detection Order"
- âœ… å€¼åº”è¯¥æ˜¯æ•´æ•°ï¼ˆ0, 1, 2, ...ï¼‰
- âœ… åŒä¸€å¼ å›¾çš„ä¸åŒå•è¯æœ‰ä¸åŒçš„order
- âœ… æ–°å›¾çš„orderä»0é‡æ–°å¼€å§‹

## ğŸ‰ æ€»ç»“

Detection OrderåŠŸèƒ½è®©æ‚¨èƒ½å¤Ÿï¼š
- âœ… è¿½è¸ªæ¯ä¸ªå•è¯çš„æ£€æµ‹é¡ºåº
- âœ… åˆ†æä½ç½®åè§å’Œç–²åŠ³æ•ˆåº”
- âœ… ç ”ç©¶è§†è§‰æ‰«ææ¨¡å¼
- âœ… è¯„ä¼°æ ‡æ³¨è´¨é‡
- âœ… å‘ç°ä¸é¡ºåºç›¸å…³çš„æ€§èƒ½é—®é¢˜

è¿™ä¸ªåŠŸèƒ½ä¸ºæ·±å…¥åˆ†æäººç±»æ ‡æ³¨è¡Œä¸ºå’Œæ¨¡å‹æ€§èƒ½æä¾›äº†æ–°çš„ç»´åº¦ï¼









