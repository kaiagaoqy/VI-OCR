{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_NUM = 5 # MNRead has 15 words at most per line\n",
    "penalty = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches_per_row(row):\n",
    "    left_side = pd.Series(row['text'].lower().split(' '))\n",
    "    right_side = pd.Series([i.lower() for i in row['rec_texts']])\n",
    "    \n",
    "    counter_left = Counter(left_side.dropna()) # dic\n",
    "    counter_right = Counter(right_side.dropna())\n",
    "    \n",
    "    # 计算两侧相同元素的个数，确保每个元素只计算一次\n",
    "    matches = sum((counter_left & counter_right).values())\n",
    "    total = sum(counter_left.values())\n",
    "    \n",
    "    # char_level\n",
    "    counter_left_char = Counter(''.join(left_side.dropna().tolist()))\n",
    "    counter_right_char = Counter(''.join(right_side.dropna().tolist()))\n",
    "    matches_char = sum((counter_left_char & counter_right_char).values())\n",
    "    total_char = sum(counter_left_char.values())\n",
    "    return [matches, total - matches, total_char-matches_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = pd.read_csv('../../data/human/SelectedFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rinse model outputs:   0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:   5%|▍         | 1/22 [00:00<00:05,  4.19it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:   9%|▉         | 2/22 [00:00<00:04,  4.56it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  14%|█▎        | 3/22 [00:00<00:04,  4.73it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  18%|█▊        | 4/22 [00:00<00:03,  4.61it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  23%|██▎       | 5/22 [00:01<00:03,  4.74it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  27%|██▋       | 6/22 [00:01<00:03,  4.78it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  32%|███▏      | 7/22 [00:01<00:03,  4.82it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  36%|███▋      | 8/22 [00:01<00:02,  4.84it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  41%|████      | 9/22 [00:01<00:02,  4.86it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  45%|████▌     | 10/22 [00:02<00:02,  4.72it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  50%|█████     | 11/22 [00:02<00:02,  4.78it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  55%|█████▍    | 12/22 [00:02<00:02,  4.80it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  59%|█████▉    | 13/22 [00:02<00:01,  4.69it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  64%|██████▎   | 14/22 [00:02<00:01,  4.53it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  68%|██████▊   | 15/22 [00:03<00:01,  4.54it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  73%|███████▎  | 16/22 [00:03<00:01,  4.58it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  77%|███████▋  | 17/22 [00:03<00:01,  4.08it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  82%|████████▏ | 18/22 [00:04<00:01,  3.81it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  86%|████████▋ | 19/22 [00:04<00:00,  3.69it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  91%|█████████ | 20/22 [00:04<00:00,  3.62it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs:  95%|█████████▌| 21/22 [00:04<00:00,  3.51it/s]/tmp/ipykernel_3436723/2929039427.py:67: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
      "/tmp/ipykernel_3436723/2929039427.py:74: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
      "Rinse model outputs: 100%|██████████| 22/22 [00:05<00:00,  4.29it/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"../../filtered/etdrs\")\n",
    "gt_dir = Path(\"../../data/etdrs\")\n",
    "files = list(output_dir.rglob('*.*'))\n",
    "result_li = []\n",
    "ba_li = []\n",
    "df_human = pd.read_csv('../../data/human/human_etdrs_acuity.csv')\n",
    "df_human.rename(columns={'acuity': 'human'}, inplace=True)\n",
    "# Load the ground truth COCO JSON (annotations only or full dataset)\n",
    "with open(gt_dir/\"anno.json\", \"r\") as f:\n",
    "    ground_truth = json.load(f)\n",
    "# Here we assume ground_truth has a key \"annotations\"\n",
    "gt_annotations = ground_truth.get(\"annotations\", [])\n",
    "img_info = ground_truth.get(\"images\", [])\n",
    "\n",
    "for model_file in tqdm(files, total=len(files), desc=\"Rinse model outputs\"):\n",
    "    # Load the model output JSON (replace with your file or object)\n",
    "    # model_file = output_dir/ \"gpt4o.json\"\n",
    "    model_name = model_file.stem\n",
    "    with open(model_file, \"r\") as f:\n",
    "        model_output = json.load(f)\n",
    "\n",
    "    # Create lookup dictionaries for ground truth annotations and image info by image_id.\n",
    "    # If there are multiple ground truth annotations per image, you might need to store a list.\n",
    "    gt_by_chart = {}\n",
    "    for info, ann in zip(img_info, gt_annotations):\n",
    "        id = ann[\"id\"]\n",
    "        full = {}\n",
    "        full[\"image_id\"] = ann[\"image_id\"]\n",
    "        full[\"file_name\"] = info[\"file_name\"]\n",
    "        full[\"Filter_no\"] = info[\"Filter_no\"]\n",
    "        full[\"text\"] = ann[\"caption\"]\n",
    "        gt_by_chart.setdefault(id, full)\n",
    "        \n",
    "    \n",
    "    # Now iterate through the model outputs and match with ground truth and image info\n",
    "    full_out = {}\n",
    "    for output_item in model_output:\n",
    "        image_id = output_item[\"image_id\"]\n",
    "        \n",
    "        # Optionally, if your model output's \"rec_texts\" is a string representation of a list,\n",
    "        # convert it to an actual list.\n",
    "        rec_texts_str = output_item.get(\"rec_texts\", \"\")\n",
    "        try:\n",
    "            rec_texts = ast.literal_eval(rec_texts_str)\n",
    "        except Exception:\n",
    "            rec_texts = rec_texts_str\n",
    "        output_item[\"rec_texts\"] = rec_texts\n",
    "\n",
    "        # Retrieve corresponding ground truth annotations (if any)\n",
    "        gt_matches = gt_by_chart.get(image_id, None)\n",
    "        if gt_matches is None:\n",
    "            continue\n",
    "        chart_no, reso, row_no = Path(gt_matches.get(\"file_name\", \" _ _ _\")).stem.split(\"_\")\n",
    "        full_out[image_id] = gt_matches\n",
    "        full_out[image_id][\"rec_texts\"] = rec_texts\n",
    "        full_out[image_id][\"chart_no\"] = chart_no\n",
    "        full_out[image_id][\"reso\"] = reso\n",
    "        full_out[image_id][\"row_no\"] = row_no\n",
    "        \n",
    "        \n",
    "    df = pd.DataFrame(full_out.values())\n",
    "    df = pd.merge(df, filter_df, on='Filter_no', how='inner')\n",
    "\n",
    "    df[['match','missing','missing_char']] = df.apply(lambda row: count_matches_per_row(row), axis=1).apply(pd.Series)\n",
    "    df['missing_clipped'] = df['missing'].clip(upper=5)\n",
    "\n",
    "    group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
    "    group_sum.columns = ['chart_no', 'a', 'b', 'VA', 'CS', 'Cond','acuity']\n",
    "    group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
    "    group_sum.columns = ['a', 'b', 'VA', 'CS', 'Cond',model_name]\n",
    "    result_li.append(group_sum)\n",
    "\n",
    "    # BA-test\n",
    "    df_ba = df.copy().groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
    "    df_ba.columns = ['chart_no', 'a', 'b', 'VA', 'CS', 'Cond','acuity']\n",
    "    df_ba = pd.merge(df_ba,df_human, on=['a', 'b'],how='left')\n",
    "    results = []\n",
    "\n",
    "    for cond, group in df_ba.groupby(['a', 'b', 'VA', 'CS', 'Cond']):\n",
    "        diff = (group['acuity'] - group['human'])\n",
    "        avg = (group['acuity'] + group['human'])/2\n",
    "        bias = diff.mean()\n",
    "        loa_low = bias - 1.96 * diff.std()\n",
    "        loa_high = bias + 1.96 * diff.std()\n",
    "            \n",
    "        results.append({\n",
    "                'Expected': cond[2], # VA\n",
    "                'Cond': cond[4],\n",
    "                'Model': model_name,\n",
    "                'Bias': bias,\n",
    "                'Diff': diff,\n",
    "                'Avg': avg,\n",
    "                'LoA_Low': loa_low,\n",
    "                'LoA_High': loa_high\n",
    "            })\n",
    "    ba_li.append(pd.DataFrame(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "summary_dir = Path(\"../../data/summary\")\n",
    "# reduce(lambda left, right: pd.merge(left, right, on=['a', 'b', 'VA', 'CS', 'Cond']), result_li).to_csv(summary_dir/'etdrs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summary_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfunctools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reduce\n\u001b[1;32m      3\u001b[0m df1 \u001b[38;5;241m=\u001b[39m reduce(\u001b[38;5;28;01mlambda\u001b[39;00m left, right: pd\u001b[38;5;241m.\u001b[39mmerge(left, right, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCond\u001b[39m\u001b[38;5;124m'\u001b[39m]), result_li)\n\u001b[0;32m----> 4\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[43msummary_dir\u001b[49m\u001b[38;5;241m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLetterSummary_0920.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      6\u001b[0m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df2[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mround(\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summary_dir' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "df1 = reduce(lambda left, right: pd.merge(left, right, on=['a', 'b', 'VA', 'CS', 'Cond']), result_li)\n",
    "df2 = pd.read_csv(summary_dir/'LetterSummary_0920.csv')\n",
    "df2['a'] = df2['a'].round(3)\n",
    "df2['b'] = df2['b'].round(3)\n",
    "df2['a'].replace(0.156, 0.157, inplace=True)\n",
    "df2['b'].replace(0.156, 0.157, inplace=True)\n",
    "df2['a'].replace(0.287, 0.288, inplace=True)\n",
    "df2['b'].replace(0.287, 0.288, inplace=True)\n",
    "df2.drop(columns=['image_no','Filter_no','human'], inplace=True)\n",
    "df2 = df2.groupby(['a', 'b']).mean().reset_index()\n",
    "\n",
    "df_human = pd.read_csv('../../data/human/human_etdrs_acuity.csv')\n",
    "df_human = df_human.rename(columns={'acuity': 'human'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(pd.merge(df1, df_human, on=['a', 'b'],how='left'), df2, on=['a', 'b'], how='left')\n",
    "models = ['human','gpt4o', 'gpt4o_mini', 'gemini_15_flash',\n",
    "       'gemini_15_pro', 'gemini_2_flash', 'claude3_7_sonnet', 'claude3_5_haiku','cogvlm', 'Qwen2.5-VL-3B-Instruct',\n",
    "       'Qwen2.5-VL-7B-Instruct', 'Qwen2.5-VL-32B-Instruct', 'qwen', 'maerec','spts', 'ppocr', 'azure', 'google',\n",
    "       'gemini-2.5-flash', 'gemini-2.5-pro', 'gpt-5-mini', 'gpt-5', 'DeepSeek-OCR_Small',\n",
    "       'DeepSeek-OCR_Base', 'DeepSeek-OCR_Tiny', 'DeepSeek-OCR_Large',\n",
    "       'DeepSeek-OCR_Gundam','seeingai']\n",
    "final_df = final_df[['a', 'b', 'VA', 'CS', 'Cond', 'Expected'] + models]\n",
    "# final_df.to_csv(summary_dir/'etdrs_combined_1021.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final_df_fig = pd.read_csv('../../data/summary/etdrs_combined_1021.csv')#final_df.copy()\n",
    "models = ['human','SeeingAI', 'gpt4o', 'gpt4o_mini', #'gemini_15_flash','gemini_15_pro',\n",
    "        'gemini_2_flash', 'claude3_7_sonnet', 'claude3_5_haiku',\n",
    "       'cogvlm','Qwen2.5-VL-3B-Instruct', 'Qwen2.5-VL-7B-Instruct', 'Qwen2.5-VL-32B-Instruct',\n",
    "       'maerec','spts', 'google', 'gemini-2.5-flash', 'gemini-2.5-pro', 'gpt-5-mini', 'gpt-5', 'DeepSeek-OCR_Small',\n",
    "       'DeepSeek-OCR_Base', 'DeepSeek-OCR_Tiny', 'DeepSeek-OCR_Large',\n",
    "       'DeepSeek-OCR_Gundam'] # 'ppocr', 'azure',\n",
    "# models = ['human','gemini-2.5-flash', 'gemini-2.5-pro','gpt-5','gpt-5-mini'] #word\n",
    "\n",
    "# comment this line to keep the original data for BA\n",
    "final_df_fig[['Expected'] + models] =  final_df_fig[['Expected'] + models]  - final_df_fig.loc[15,['Expected'] + models]\n",
    "final_df_fig.loc[final_df_fig['Cond']=='Original', 'Cond'] = 'Combined'\n",
    "\n",
    "model_name_dict = {'human':'Human','SeeingAI':'SeeingAI', 'gpt4o':'GPT4O', 'gpt4o_mini':'GPT4O Mini', \n",
    "                   'gemini_15_flash':'Gemini-1.5 Flash','gemini_15_pro':'Gemini-1.5 Pro', 'gemini_2_flash':'Gemini-2 Flash', \n",
    "                   'claude3_7_sonnet':'Claude3.7 Sonnet', 'claude3_5_haiku':'Claude3.5 Haiku','cogvlm':'CogAgent',\n",
    "                   'Qwen2.5-VL-3B-Instruct':'Qwen2.5-VL-3B', 'Qwen2.5-VL-7B-Instruct':'Qwen2.5-VL-7B', 'Qwen2.5-VL-32B-Instruct':'Qwen2.5-VL-32B',\n",
    "                   'maerec':'DBNet++ & MaeRec','spts':'SPTS v2', 'ppocr':'PPOCR', 'azure':'Azure', 'google':'Google Vision',\n",
    "                   'gemini-2.5-flash':'Gemini-2.5 Flash','gemini-2.5-pro':'Gemini-2.5 Pro', 'gpt-5-mini':'GPT-5 Mini', 'gpt-5':'GPT-5', 'DeepSeek-OCR_Small':'DeepSeek-OCR Small',\n",
    "                   'DeepSeek-OCR_Base':'DeepSeek-OCR Base', 'DeepSeek-OCR_Tiny':'DeepSeek-OCR Tiny', 'DeepSeek-OCR_Large':'DeepSeek-OCR Large',\n",
    "                   'DeepSeek-OCR_Gundam':'DeepSeek-OCR Gundam', 'seeingai':'SeeingAI'}\n",
    "final_df_fig.rename(columns=model_name_dict, inplace=True)\n",
    "models = [model_name_dict[i] for i in models if i in model_name_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "sns.set_theme(style=\"white\")\n",
    "# Define a function to choose marker based on condition\n",
    "def get_marker(cond):\n",
    "    if cond == 'Horizontal':\n",
    "        return 'o'  # hollow square\n",
    "    elif cond == 'Vertical':\n",
    "        return '^'  # hollow triangle\n",
    "    else:\n",
    "        return 's'  # hollow circle\n",
    "\n",
    "# Create a color palette for unique conditions\n",
    "unique_conditions = final_df_fig['Cond'].unique()\n",
    "# palette = sns.color_palette(\"viridis\", len(unique_conditions))\n",
    "# color_mapping = {cond: palette[i] for i, cond in enumerate(unique_conditions)}\n",
    "colors = ['orange', 'black', '#6A9C89', 'black']\n",
    "# Create the mapping\n",
    "color_mapping = {cond: color for cond, color in zip(unique_conditions, colors)}\n",
    "\n",
    "# Define a function to plot a scatter plot on a given axis for a specified model column\n",
    "def plot_scatter(ax, model_col, df):\n",
    "    # Add a dashed diagonal line that spans the range of the data\n",
    "    x_vals = df['Expected']\n",
    "    y_vals = df[model_col]\n",
    "    overall_min = -0.1#min(x_vals.min(), y_vals.min())\n",
    "    overall_max = 2.0#max(x_vals.max(), y_vals.max())\n",
    "    ax.plot([overall_min, overall_max], [overall_min, overall_max],\n",
    "            linestyle='--', color='grey', linewidth=2.5)\n",
    "    x = np.linspace(overall_min, overall_max, 100)\n",
    "    # Fill the area between y = x - 0.2 and y = x + 0.2\n",
    "    ax.fill_between(x, x - 0.2, x + 0.2, color='grey', alpha=0.25)\n",
    "    \n",
    "    # Plot each group (by Cond) separately to assign markers and colors\n",
    "    for cond, group in df.groupby('Cond'):\n",
    "        marker = get_marker(cond)\n",
    "        ax.scatter(\n",
    "            group['Expected'],\n",
    "            group[model_col],\n",
    "            label=cond,\n",
    "            marker=marker,\n",
    "            facecolors=color_mapping[cond],  # makes marker hollow\n",
    "            edgecolors=color_mapping[cond],\n",
    "            s=100  # marker size\n",
    "        )\n",
    "        \n",
    "    # calulate the correlation\n",
    "    from scipy.stats import linregress\n",
    "    slope, intercept, r_value, p_value, std_err = linregress(x_vals.astype(float).to_numpy(), y_vals.astype(float).to_numpy())\n",
    "    r_squared = r_value ** 2\n",
    "    # Annotate R² and p-value in bottom-right\n",
    "    ax.text(0.96, 0.02, f'$R^2$ = {r_squared:.2f}, '+ (f'p = {p_value:.3f}' if p_value > 0.001 else 'p < 0.001'),\n",
    "            ha='right', va='bottom', transform=ax.transAxes,\n",
    "            fontsize=11)\n",
    "\n",
    "    \n",
    "    # Set labels, title and legend\n",
    "    ax.set_xlabel(\"Expected Acuity Change (logMAR)\")\n",
    "    ax.set_ylabel(\"Measured Acuity Change (logMAR)\")\n",
    "    ax.set_xlim(-0.1, 2.0)\n",
    "    ax.set_ylim(-0.1, 2.0)\n",
    "    ax.set_title(f\"{model_col}\", fontsize=16)\n",
    "    # ax.legend()\n",
    "\n",
    "# List of models to plot (include 'human' and other models)\n",
    "# models = ['human','SeeingAI', 'gpt4o', 'gpt4o_mini', 'gemini_15_flash',\n",
    "#        'gemini_15_pro', 'gemini_2_flash', 'claude3_7_sonnet', 'claude3_5_haiku',\n",
    "#        'cogvlm','Qwen2.5-VL-3B-Instruct', 'Qwen2.5-VL-7B-Instruct', 'Qwen2.5-VL-32B-Instruct',\n",
    "#        'qwen', 'maerec','spts', 'ppocr', 'azure', 'google']\n",
    "\n",
    "# Determine subplot grid layout; here we choose 2 columns\n",
    "num_models = len(models)\n",
    "ncols = 5\n",
    "nrows = math.ceil(num_models / ncols)\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(3 * ncols, 3 * nrows))\n",
    "# If there's only one subplot, axes is not an array; make it a list for consistency.\n",
    "if num_models == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "# Plot each model in its corresponding subplot\n",
    "for i, model in enumerate(models):\n",
    "    plot_scatter(axes[i], model, final_df_fig)\n",
    "\n",
    "# Remove any extra subplots if the grid has more axes than models\n",
    "for j in range(i+1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "    \n",
    "handles, labels = axes.flatten()[0].get_legend_handles_labels()\n",
    "\n",
    "# Place a common legend outside the figure. Here, loc='upper center' puts it at the top,\n",
    "# and bbox_to_anchor adjusts its position. Adjust ncol for number of columns in legend.\n",
    "# fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=3, fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figure/etdrs.pdf', dpi=300)\n",
    "\n",
    "# # Plot for 'human'\n",
    "# for model in models:\n",
    "#     plot_scatter(model, final_df_fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(arr):\n",
    "    return np.sqrt(np.mean(arr ** 2))\n",
    "std_values = final_df_fig[models].sub(final_df_fig['Expected'], axis=0).apply(rmse, axis=0)\n",
    "# Identify the model with the smallest std\n",
    "min_model = std_values.idxmin()\n",
    "\n",
    "# Create a color list: red for the model with the smallest std; use a nice color (e.g. 'skyblue') for others.\n",
    "colors = ['#D76C82' if model == min_model else 'gray' for model in std_values.index]\n",
    "\n",
    "# Optional: sort the values for a prettier display (and sort colors accordingly)\n",
    "std_values_sorted = std_values.sort_values(ascending=False)\n",
    "\n",
    "colors_sorted = [colors[std_values.index.get_loc(model)] for model in std_values_sorted.index]\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "std_values_sorted.plot(kind='bar', color=colors_sorted, edgecolor='none', width=0.9, ax=ax)\n",
    "\n",
    "for patch, tick in zip(ax.patches, ax.get_xticklabels()):\n",
    "    if tick.get_text().lower() == 'human':  # adjust case if needed\n",
    "        # Set no fill (hollow)\n",
    "        patch.set_facecolor('none')\n",
    "        # Set dashed edge\n",
    "        patch.set_linestyle('--')\n",
    "        patch.set_linewidth(2)\n",
    "        # Optionally, set a specific edge color. For instance, if you want red:\n",
    "        patch.set_edgecolor('#D76C82')\n",
    "# Customize axis labels, title, and tick parameters\n",
    "# plt.xlabel(\"Model\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Root Mean Square Error\", fontsize=14)\n",
    "plt.yticks(np.arange(0,1.01,0.25),fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.tight_layout()\n",
    "# plt.savefig('../../figure/etdrs_rmse.pdf', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(arr):\n",
    "    return np.sqrt(np.mean(arr ** 2))\n",
    "std_values = final_df_fig[models[1:]].sub(final_df_fig['Human'], axis=0).apply(rmse, axis=0)\n",
    "# Identify the model with the smallest std\n",
    "min_model = std_values.idxmin()\n",
    "\n",
    "# Create a color list: red for the model with the smallest std; use a nice color (e.g. 'skyblue') for others.\n",
    "colors = ['green' if model == min_model else 'gray' for model in std_values.index]\n",
    "\n",
    "# Optional: sort the values for a prettier display (and sort colors accordingly)\n",
    "std_values_sorted = std_values.sort_values(ascending=False)\n",
    "\n",
    "colors_sorted = [colors[std_values.index.get_loc(model)] for model in std_values_sorted.index]\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "std_values_sorted.plot(kind='bar', color=colors_sorted, edgecolor='none', width=0.9, ax=ax)\n",
    "\n",
    "# Customize axis labels, title, and tick parameters\n",
    "# plt.xlabel(\"Model\", fontsize=14, fontweight='bold')\n",
    "plt.ylabel(\"Root Mean Square Error\", fontsize=14)\n",
    "plt.yticks(np.arange(0,1.01,0.25),fontsize=12)\n",
    "plt.xticks(rotation=75, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figure/etdrs_rmse_tohuman.pdf', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "def rmse(arr):\n",
    "    return np.sqrt(np.mean(arr ** 2))\n",
    "\n",
    "# --- metrics ---\n",
    "rmse_vs_exp = final_df_fig[models].sub(final_df_fig['Expected'], axis=0).apply(rmse, axis=0)\n",
    "models_wo_human = [m for m in models if str(m).casefold() != 'human']\n",
    "rmse_to_human = final_df_fig[models_wo_human].sub(final_df_fig['Human'], axis=0).apply(rmse, axis=0)\n",
    "\n",
    "# --- order & align (ascending by left) ---\n",
    "order = rmse_vs_exp.sort_values(ascending=True).index.tolist()\n",
    "left_vals  = rmse_vs_exp.reindex(order).values\n",
    "right_vals = rmse_to_human.reindex(order).values\n",
    "\n",
    "y = np.arange(len(order))\n",
    "bar_h = 0.58\n",
    "\n",
    "# --- colors & highlights ---\n",
    "min_left_label  = order[np.nanargmin(left_vals)]\n",
    "min_right_label = rmse_to_human.idxmin() if len(rmse_to_human) else None\n",
    "left_colors  = ['#D76C82' if lab == min_left_label else '#9AA0A6' for lab in order]\n",
    "right_colors = []\n",
    "for lab, v in zip(order, right_vals):\n",
    "    if np.isnan(v) or str(lab).casefold() == 'human':\n",
    "        right_colors.append('none')\n",
    "    elif lab == min_right_label:\n",
    "        right_colors.append('#2E7D32')\n",
    "    else:\n",
    "        right_colors.append('#C2A68C')\n",
    "\n",
    "# --- axis range & central gap ---\n",
    "lim_left  = float(np.nanmax(left_vals))\n",
    "lim_right = float(np.nanmax(right_vals[~np.isnan(right_vals)]) if np.any(~np.isnan(right_vals)) else 0.0)\n",
    "lim = max(lim_left, lim_right, 1.0)\n",
    "step = 0.25\n",
    "max_tick = np.ceil(lim / step) * step\n",
    "\n",
    "gap = max_tick * 0.22   # 中缝宽度的一半（可调：0.12~0.25）\n",
    "left_anchor  = -gap      # 左柱起点在 -gap 处向左延伸\n",
    "right_anchor =  gap      # 右柱起点在 +gap 处向右延伸\n",
    "\n",
    "fig_h = max(6, 0.45 * len(order) + 2)\n",
    "fig, ax = plt.subplots(figsize=(12, fig_h))\n",
    "\n",
    "# 背景/网格\n",
    "ax.set_facecolor('#FCFCFD')\n",
    "# ax.grid(axis='x', linestyle='--', linewidth=0.6, alpha=0.5)\n",
    "for s in ['top', 'right', 'left']: ax.spines[s].set_visible(False)\n",
    "\n",
    "# --- bars (外移) ---\n",
    "# 左：从 -gap 向左画，width 用正值，left=-(gap+value)\n",
    "left_bars = ax.barh(\n",
    "    y, left_vals, height=bar_h, color=left_colors, edgecolor='none',\n",
    "    left=-(gap + left_vals), zorder=3\n",
    ")\n",
    "# Human 左侧空心\n",
    "for patch, lab in zip(left_bars, order):\n",
    "    if str(lab).casefold() == 'human':\n",
    "        patch.set_facecolor('none'); patch.set_edgecolor('#D76C82')\n",
    "        patch.set_linestyle('--'); patch.set_linewidth(1.8)\n",
    "\n",
    "# 右：从 +gap 向右画\n",
    "right_bars = ax.barh(\n",
    "    y, np.nan_to_num(right_vals, nan=0.0), height=bar_h,\n",
    "    color=right_colors, edgecolor='none', left=right_anchor, zorder=3\n",
    ")\n",
    "# 隐藏 Human/NaN 右半\n",
    "for patch, lab, v in zip(right_bars, order, right_vals):\n",
    "    if np.isnan(v) or str(lab).casefold() == 'human':\n",
    "        patch.set_visible(False)\n",
    "\n",
    "# --- 中缝与标题 ---\n",
    "ax.axvspan(-gap, gap, color='white', alpha=0.95, zorder=1)  # 中间留白带\n",
    "ax.text(-gap, 1.012, 'vs Expected',            transform=ax.get_xaxis_transform(), ha='right', va='bottom',\n",
    "        fontsize=12, fontweight='bold', color='#3A3F44')\n",
    "ax.text( gap, 1.012, 'vs Human',   transform=ax.get_xaxis_transform(), ha='left',  va='bottom',\n",
    "        fontsize=12, fontweight='bold', color='#3A3F44')\n",
    "\n",
    "# --- model names 放在中缝里（不与柱子重合） ---\n",
    "for yi, lab in zip(y, order):\n",
    "    ax.text(0, yi, str(lab), ha='center', va='center', fontsize=11.2, color='#28323C', zorder=4)\n",
    "\n",
    "# --- x 轴：对称范围 + 绝对值刻度 ---\n",
    "xmax = max_tick + gap + max_tick*0.05\n",
    "ax.set_xlim(-xmax, xmax)\n",
    "ax.xaxis.set_major_locator(MultipleLocator(step))\n",
    "ax.xaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.set_xticklabels([t.get_text().lstrip('-') for t in ax.get_xticklabels()], fontsize=13)\n",
    "ax.set_xlabel('Root Mean Square Error', fontsize=15.5, color='#3A3F44')\n",
    "\n",
    "# 移除 ytick 文本（我们用中缝标签）\n",
    "ax.set_yticks(y); ax.set_yticklabels([]); ax.tick_params(axis='y', length=0)\n",
    "\n",
    "# --- 值标签（条末端） ---\n",
    "pad = max_tick * 0.015\n",
    "for v, b in zip(left_vals, left_bars):\n",
    "    if v <= 0: continue\n",
    "    x = -gap - v - pad\n",
    "    ax.text(x, b.get_y()+b.get_height()/2, f'{v:.2f}'.rstrip('0').rstrip('.'),\n",
    "            ha='right', va='center', fontsize=12, color='#4A4F55', zorder=4)\n",
    "for v, b in zip(right_vals, right_bars):\n",
    "    if np.isnan(v) or v <= 0: continue\n",
    "    x =  gap + v + pad\n",
    "    ax.text(x, b.get_y()+b.get_height()/2, f'{v:.2f}'.rstrip('0').rstrip('.'),\n",
    "            ha='left', va='center', fontsize=12, color='#4A4F55', zorder=4)\n",
    "\n",
    "# --- 图例 ---\n",
    "legend_handles = [\n",
    "    Patch(facecolor='#D76C82', edgecolor='none', label='Min RMSE (vs Expected)'),\n",
    "    Patch(facecolor='#2E7D32', edgecolor='none', label='Min RMSE (vs Human)'),\n",
    "    # Patch(facecolor='#9AA0A6', edgecolor='none', label='Others (left)'),\n",
    "    # Patch(facecolor='#C2A68C', edgecolor='none', label='Others (right)'),\n",
    "    Patch(facecolor='none', edgecolor='#D76C82', linestyle='--', label='Human'),\n",
    "]\n",
    "ax.legend(handles=legend_handles, frameon=False, fontsize=12.5, ncol=3, loc='upper center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../figure/etdrs_rmse_stacked.pdf', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cis/home/qgao14/my_documents/VIOCR_infer_models/py39/lib/python3.9/site-packages/numpy/_core/_methods.py:218: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/cis/home/qgao14/my_documents/VIOCR_infer_models/py39/lib/python3.9/site-packages/numpy/_core/_methods.py:210: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# def make_long_ba_df(ba_df_wide):\n",
    "#     records = []\n",
    "#     for _, row in ba_df_wide.iterrows():\n",
    "#         diff = np.asarray(row[\"Diff\"]).ravel()\n",
    "#         avg  = np.asarray(row[\"Avg\"]).ravel()\n",
    "#         L = min(len(diff), len(avg))\n",
    "#         if L == 0:\n",
    "#             continue\n",
    "#         # 逐样本展开\n",
    "#         rec = pd.DataFrame({\n",
    "#             \"Model\":    row[\"Model\"],\n",
    "#             \"Cond\":     row[\"Cond\"],\n",
    "#             \"Expected_logMAR\": row[\"Expected_logMAR\"],\n",
    "#             \"Expected\": avg[:L].astype(float),\n",
    "#             \"Bias\":     diff[:L].astype(float),\n",
    "#         })\n",
    "#         records.append(rec)\n",
    "\n",
    "#     long_df = pd.concat(records, ignore_index=True).dropna(subset=[\"Expected\",\"Bias\"])\n",
    "\n",
    "#     # 每 (Model, Cond) 计算 BA 统计并回填\n",
    "#     stats = (\n",
    "#         long_df.groupby([\"Model\",\"Cond\",\"Expected_logMAR\"])[\"Bias\"]\n",
    "#         .agg(n=\"size\", mean=\"mean\", std=lambda s: np.nanstd(s, ddof=1))\n",
    "#         .reset_index()\n",
    "#         .rename(columns={\"mean\":\"mean_bias\",\"std\":\"std_bias\"})\n",
    "#     )\n",
    "#     stats[\"LoA_Low\"]  = stats[\"mean_bias\"] - 1.96 * stats[\"std_bias\"]\n",
    "#     stats[\"LoA_High\"] = stats[\"mean_bias\"] + 1.96 * stats[\"std_bias\"]\n",
    "#     stats[\"LoA_Width\"] = stats[\"LoA_High\"] - stats[\"LoA_Low\"]\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "def make_long_ba_df(ba_df_wide):\n",
    "    records = []\n",
    "    for i, row in ba_df_wide.iterrows():\n",
    "        diff = np.asarray(row[\"Diff\"]).ravel()\n",
    "        avg  = np.asarray(row[\"Avg\"]).ravel()\n",
    "        L = min(len(diff), len(avg))\n",
    "        if L == 0:\n",
    "            continue\n",
    "        mean_avg = avg.mean().astype(float)\n",
    "        mean_diff = diff.mean().astype(float)\n",
    "        # 逐样本展开\n",
    "        rec = pd.DataFrame(data={\n",
    "            \"Model\":    row[\"Model\"],\n",
    "            \"Cond\":     row[\"Cond\"],\n",
    "            \"Expected_logMAR\": row[\"Expected_logMAR\"],\n",
    "            \"Expected\": mean_avg,\n",
    "            \"Bias\":     mean_diff,\n",
    "            \"LoA_Low\":  mean_diff - 1.96 * np.std(diff, ddof=1),\n",
    "            \"LoA_High\": mean_diff + 1.96 * np.std(diff, ddof=1),\n",
    "            \"LoA_Width\": 2 * 1.96 * np.std(diff, ddof=1),\n",
    "        },index=[i])\n",
    "        records.append(rec)\n",
    "\n",
    "    long_df = pd.concat(records, ignore_index=True).dropna(subset=[\"Expected\",\"Bias\"])\n",
    "\n",
    "    # 每 (Model, Cond) 计算 BA 统计并回填\n",
    "    # stats = (\n",
    "    #     long_df.groupby([\"Model\",\"Cond\",\"Expected_logMAR\"])[\"Bias\"]\n",
    "    #     .agg(n=\"size\", mean=\"mean\", std=lambda s: np.nanstd(s, ddof=1))\n",
    "    #     .reset_index()\n",
    "    #     .rename(columns={\"mean\":\"mean_bias\",\"std\":\"std_bias\"})\n",
    "    # )\n",
    "    # stats[\"LoA_Low\"]  = stats[\"mean_bias\"] - 1.96 * stats[\"std_bias\"]\n",
    "    # stats[\"LoA_High\"] = stats[\"mean_bias\"] + 1.96 * stats[\"std_bias\"]\n",
    "    # stats[\"LoA_Width\"] = stats[\"LoA_High\"] - stats[\"LoA_Low\"]\n",
    "    # out = long_df.merge(stats[[\"Model\",\"Cond\",\"Expected_logMAR\",\"LoA_Low\",\"LoA_High\",\"LoA_Width\"]], on=[\"Model\",\"Cond\",\"Expected_logMAR\"], how=\"left\")\n",
    "    return long_df\n",
    "ba_df = pd.concat(ba_li, axis=0)\n",
    "ba_df['Expected'] = ba_df['Expected'] - (-0.24)\n",
    "ba_df['Expected_logMAR'] = ba_df['Expected'] - 0.24\n",
    "ba_df.drop(columns=['Expected'], inplace=True)\n",
    "ba_df[\"LoA_Width\"] = ba_df[\"LoA_High\"] - ba_df[\"LoA_Low\"]\n",
    "ba_df['Model'] = ba_df['Model'].map(model_name_dict).fillna(ba_df['Model'])\n",
    "ba_df = pd.merge(ba_df.groupby(['Model','Cond','Expected_logMAR'])['Diff'].apply(lambda x: np.concatenate(x.dropna().values,axis=0)).reset_index(), \n",
    "ba_df.groupby(['Model','Cond','Expected_logMAR'])['Avg'].apply(lambda x: np.concatenate(x.dropna().values,axis=0)).reset_index(), on=['Model','Cond','Expected_logMAR'])\n",
    "# ========= 展开并计算 LoA =========\n",
    "ba_df = make_long_ba_df(ba_df)   # 注意：这里覆盖变量名，后续绘图直接用 ba_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "# =====================================================\n",
    "# 0) 将你的 ba_df(含 list) 展开为逐样本，同时计算 LoA\n",
    "#     输入列:  Model, Cond, Diff(list), Avg(list)\n",
    "#     输出列:  Model, Cond, Expected, Bias, LoA_Low, LoA_High, LoA_Width\n",
    "# =====================================================\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 1) 你的 marker / 颜色 映射（原样保留）\n",
    "# =====================================================\n",
    "def get_marker(cond):\n",
    "    if cond == 'Horizontal':\n",
    "        return 'o'  # hollow square (注释来自你原代码，这里沿用; 实际是圆点标记)\n",
    "    elif cond == 'Vertical':\n",
    "        return '^'  # hollow triangle\n",
    "    elif cond == 'Original':\n",
    "        return 'x'  # \n",
    "    else:\n",
    "        return 's'  # hollow circle\n",
    "\n",
    "# models = ba_df['Model'].unique()\n",
    "\n",
    "unique_conditions = ba_df['Cond'].unique()\n",
    "colors = ['black', 'orange', 'red', '#6A9C89']  # 你给定的颜色序列\n",
    "# 若条件数超过颜色数，做一个稳妥的扩展\n",
    "if len(unique_conditions) > len(colors):\n",
    "    extra = sns.color_palette(\"tab10\", len(unique_conditions) - len(colors))\n",
    "    colors = list(colors) + list(extra)\n",
    "color_mapping = {cond: color for cond, color in zip(unique_conditions, colors)}\n",
    "\n",
    "# =====================================================\n",
    "# 2) 数据准备（与你的代码一致）\n",
    "# =====================================================\n",
    "try:\n",
    "    ba_df[\"_ExpectedNum_\"] = pd.to_numeric(ba_df[\"Expected\"], errors=\"raise\")\n",
    "    x_col = \"_ExpectedNum_\"\n",
    "    x_tick_labels_from = \"Expected\"\n",
    "except Exception:\n",
    "    x_col = \"Expected\"\n",
    "    x_tick_labels_from = \"Expected\"\n",
    "\n",
    "cond_list = list(ba_df[\"Cond\"].dropna().unique())\n",
    "palette = color_mapping\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "# =====================================================\n",
    "# 3) FacetGrid（与你的结构一致；仅把 marker 接入）\n",
    "# =====================================================\n",
    "g = sns.FacetGrid(\n",
    "    ba_df.sort_values([\"Model\", x_col]),\n",
    "    col=\"Model\",\n",
    "    col_wrap=5,\n",
    "    height=4.0,\n",
    "    sharey=True,\n",
    "    margin_titles=True,\n",
    ")\n",
    "\n",
    "def plot_ba_by_cond(data, color=None, **kws):\n",
    "    ax = plt.gca()\n",
    "    data = data.sort_values([x_col, \"Cond\"])\n",
    "    x_min = -.15\n",
    "    x_max = 1.8\n",
    "    mu = float(np.nanmean(data[\"Bias\"]))\n",
    "    sd = float(np.nanstd(data[\"Bias\"], ddof=1)) if data[\"Bias\"].size > 1 else 0.0\n",
    "    lo, hi = mu - 1.96*sd, mu + 1.96*sd\n",
    "    ax.hlines([mu], x_min, x_max, colors='#C96868', linestyles=\"-\",  linewidth=1.6)\n",
    "    ax.hlines([lo, hi], x_min, x_max, colors='#C96868', linestyles=\":\", linewidth=1.2)\n",
    "    for cond in cond_list:\n",
    "        d = data[data[\"Cond\"] == cond]\n",
    "        if d.empty:\n",
    "            continue\n",
    "        c = palette[cond]\n",
    "        m = get_marker(cond)  # <<<<<< 接入你的 marker 规则\n",
    "\n",
    "        # # LoA 阴影带（本实现中 LoA 对应 (Model,Cond) 常数，填充为平行带）\n",
    "        # ax.fill_between(\n",
    "        #     d[x_col],\n",
    "        #     d[\"LoA_Low\"],\n",
    "        #     d[\"LoA_High\"],\n",
    "        #     alpha=0.14, color=c, linewidth=0\n",
    "        # )\n",
    "        mu = float(np.nanmean(d[\"Bias\"]))\n",
    "        sd = float(np.nanstd(d[\"Bias\"], ddof=1)) if d[\"Bias\"].size > 1 else 0.0\n",
    "        lo, hi = mu - 1.96*sd, mu + 1.96*sd\n",
    "        # if cond != 'Original':\n",
    "        #     ax.hlines([mu], x_min, x_max, colors=c, linestyles=\"-\",  linewidth=1.6)\n",
    "        #     ax.hlines([lo, hi], x_min, x_max, colors=c, linestyles=\":\", linewidth=1.2)\n",
    "        # Bias 折线 + 散点（marker 按照你的规则）\n",
    "        ax.scatter(d[x_col], d[\"Bias\"], marker=m, color=c, label=cond, zorder=3, s=38)\n",
    "        # ax.scatter(d[x_col], d[\"Bias\"], s=15, color=c, edgecolor=\"white\", linewidth=1., zorder=4, marker=m)\n",
    "\n",
    "\n",
    "\n",
    "    # y=0 基线\n",
    "    ax.hlines([0], x_min, x_max, linestyle=\"--\", linewidth=1, color=\"0.35\", zorder=1)\n",
    "\n",
    "    # ——统计面板（与你的注释逻辑一致）——\n",
    "    stats = (data\n",
    "             .groupby(\"Cond\")\n",
    "             .agg(n=(\"Bias\",\"size\"),\n",
    "                  mean_bias=(\"Bias\",\"mean\"),\n",
    "                  std_bias=(\"Bias\",\"std\"),\n",
    "                  mean_width=(\"LoA_Width\",\"mean\"))\n",
    "             .reset_index())\n",
    "    stats[\"se\"] = stats[\"std_bias\"] / np.sqrt(stats[\"n\"]).replace(0, np.nan)\n",
    "    stats[\"ci95\"] = 1.96 * stats[\"se\"]\n",
    "    # stats = stats.sort_values(\"mean_bias\", key=lambda s: s.abs(), ascending=False)\n",
    "    stats = stats.sort_values(\"Cond\", ascending=False)\n",
    "    y_pos = 0.22\n",
    "    line_height = 0.08\n",
    "    for _, r in stats.iterrows():\n",
    "        if r[\"n\"] <= 1:\n",
    "            continue\n",
    "        cond = r[\"Cond\"]\n",
    "        c = palette.get(cond, \"black\")\n",
    "        ci_text = f\" ±{r['ci95']:.2f}\" if np.isfinite(r[\"ci95\"]) else \"\"\n",
    "        line = f\"{cond}: μBias={r['mean_bias']:.2f}{ci_text}\"\n",
    "        ax.text(\n",
    "            0.04, y_pos, line,\n",
    "            transform=ax.transAxes, va=\"top\", ha=\"left\",\n",
    "            fontsize=12, color=c, \n",
    "        )\n",
    "        y_pos -= line_height\n",
    "\n",
    "    # 轴与网格\n",
    "    ax.set_xlabel(\"Avg Performance\")\n",
    "    ax.set_ylabel(\"Bias (Model - Human)\")\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter(\"%.2f\"))\n",
    "    ax.set_ylim(-1.5, 1)\n",
    "    ax.grid(alpha=0.25)\n",
    "\n",
    "    # 若 Expected 是数值列，刻度格式化\n",
    "    if x_col == \"_ExpectedNum_\":\n",
    "        ticks = np.linspace(data[x_col].min(), data[x_col].max(), num=5)\n",
    "        ax.set_xticks(ticks)\n",
    "        ax.set_xticklabels([f\"{t:.2f}\" for t in ticks], rotation=30, ha=\"right\")\n",
    "\n",
    "g.map_dataframe(plot_ba_by_cond)\n",
    "\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "\n",
    "# 统一图例\n",
    "handles, labels = g.axes.flat[0].get_legend_handles_labels()\n",
    "uniq = dict(zip(labels, handles))\n",
    "g.fig.legend(\n",
    "    uniq.values(), uniq.keys(),\n",
    "    loc=\"upper center\",\n",
    "    bbox_to_anchor=(0.5, 1.08),\n",
    "    ncol=min(len(uniq), 5),\n",
    "    frameon=True,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "sns.despine(trim=True)\n",
    "# plt.show()\n",
    "plt.savefig('../../figure/etdrs_ba.pdf', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lookup dictionaries for ground truth annotations and image info by image_id.\n",
    "# If there are multiple ground truth annotations per image, you might need to store a list.\n",
    "gt_by_chart = {}\n",
    "for info, ann in zip(img_info, gt_annotations):\n",
    "    id = ann[\"id\"]\n",
    "    full = {}\n",
    "    full[\"image_id\"] = ann[\"image_id\"]\n",
    "    full[\"file_name\"] = info[\"file_name\"]\n",
    "    full[\"Filter_no\"] = info[\"Filter_no\"]\n",
    "    full[\"text\"] = ann[\"caption\"]\n",
    "    gt_by_chart.setdefault(id, full)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "# Now iterate through the model outputs and match with ground truth and image info\n",
    "full_out = {}\n",
    "for output_item in model_output:\n",
    "    image_id = output_item[\"image_id\"]\n",
    "    \n",
    "    # Optionally, if your model output's \"rec_texts\" is a string representation of a list,\n",
    "    # convert it to an actual list.\n",
    "    rec_texts_str = output_item.get(\"rec_texts\", \"\")\n",
    "    try:\n",
    "        rec_texts = ast.literal_eval(rec_texts_str)\n",
    "    except Exception:\n",
    "        rec_texts = rec_texts_str\n",
    "    output_item[\"rec_texts\"] = rec_texts\n",
    "\n",
    "    # Retrieve corresponding ground truth annotations (if any)\n",
    "    gt_matches = gt_by_chart.get(image_id, {})\n",
    "    chart_no, reso, row_no = Path(gt_matches.get(\"file_name\", \"___\")).stem.split(\"_\")\n",
    "    full_out[image_id] = gt_matches\n",
    "    full_out[image_id][\"rec_texts\"] = rec_texts\n",
    "    full_out[image_id][\"chart_no\"] = chart_no\n",
    "    full_out[image_id][\"reso\"] = reso\n",
    "    full_out[image_id][\"row_no\"] = row_no\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def count_matches_per_row(row):\n",
    "    left_side = pd.Series(row['text'].lower().split(' '))\n",
    "    right_side = pd.Series([i.lower() for i in row['rec_texts']])\n",
    "    \n",
    "    counter_left = Counter(left_side.dropna()) # dic\n",
    "    counter_right = Counter(right_side.dropna())\n",
    "    \n",
    "    # 计算两侧相同元素的个数，确保每个元素只计算一次\n",
    "    matches = sum((counter_left & counter_right).values())\n",
    "    total = sum(counter_left.values())\n",
    "    \n",
    "    # char_level\n",
    "    counter_left_char = Counter(''.join(left_side.dropna().tolist()))\n",
    "    counter_right_char = Counter(''.join(right_side.dropna().tolist()))\n",
    "    matches_char = sum((counter_left_char & counter_right_char).values())\n",
    "    total_char = sum(counter_left_char.values())\n",
    "    return [matches, total - matches, total_char-matches_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2338998/3606988138.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*0.01).reset_index()\n",
      "/tmp/ipykernel_2338998/3606988138.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>VA</th>\n",
       "      <th>CS</th>\n",
       "      <th>Cond</th>\n",
       "      <th>gpt4o</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.016</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.49</td>\n",
       "      <td>Horizontal</td>\n",
       "      <td>1.796667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.59</td>\n",
       "      <td>Combined</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.031</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1.67</td>\n",
       "      <td>Horizontal</td>\n",
       "      <td>1.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.82</td>\n",
       "      <td>Combined</td>\n",
       "      <td>1.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.063</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.85</td>\n",
       "      <td>Horizontal</td>\n",
       "      <td>1.556667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.086</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.07</td>\n",
       "      <td>Combined</td>\n",
       "      <td>1.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.125</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.96</td>\n",
       "      <td>Horizontal</td>\n",
       "      <td>1.253333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.33</td>\n",
       "      <td>Combined</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.250</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.36</td>\n",
       "      <td>2.05</td>\n",
       "      <td>Horizontal</td>\n",
       "      <td>0.957500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.288</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.59</td>\n",
       "      <td>Combined</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.20</td>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.47</td>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.76</td>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.913333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.089</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>1.06</td>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.936667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.355</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>1.65</td>\n",
       "      <td>Vertical</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>2.13</td>\n",
       "      <td>Original</td>\n",
       "      <td>0.917500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        a      b    VA    CS        Cond     gpt4o\n",
       "0   0.016  1.000  1.57  1.49  Horizontal  1.796667\n",
       "1   0.027  0.027  1.57  0.59    Combined  1.800000\n",
       "2   0.031  1.000  1.27  1.67  Horizontal  1.766667\n",
       "3   0.048  0.048  1.27  0.82    Combined  1.790000\n",
       "4   0.063  1.000  0.97  1.85  Horizontal  1.556667\n",
       "5   0.086  0.086  0.97  1.07    Combined  1.560000\n",
       "6   0.125  1.000  0.66  1.96  Horizontal  1.253333\n",
       "7   0.157  0.157  0.66  1.33    Combined  1.390000\n",
       "8   0.250  1.000  0.36  2.05  Horizontal  0.957500\n",
       "9   0.288  0.288  0.36  1.59    Combined  1.000000\n",
       "10  1.000  0.011  0.13  0.20    Vertical  0.960000\n",
       "11  1.000  0.022  0.03  0.47    Vertical  0.950000\n",
       "12  1.000  0.045 -0.04  0.76    Vertical  0.913333\n",
       "13  1.000  0.089 -0.10  1.06    Vertical  0.936667\n",
       "14  1.000  0.355 -0.19  1.65    Vertical  0.920000\n",
       "15  1.000  1.000 -0.24  2.13    Original  0.917500"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.DataFrame(full_out.values())\n",
    "df = pd.merge(df, filter_df, on='Filter_no', how='inner')\n",
    "# df['text'].str.split(\" \", expand=True)\n",
    "# # Split the 'text' column into separate columns using space as the delimiter\n",
    "# split_text = df['text'].str.split(\" \", expand=True)\n",
    "\n",
    "# # Rename the split columns to L1, L2, ..., Ln\n",
    "# split_text.columns = [f\"L{i+1}\" for i in range(split_text.shape[1])]\n",
    "\n",
    "# # Concatenate the new columns back to the original DataFrame\n",
    "# df = pd.concat([df, split_text], axis=1)\n",
    "df[['match','missing','missing_char']] = df.apply(lambda row: count_matches_per_row(row), axis=1).apply(pd.Series)\n",
    "df['missing_clipped'] = df['missing'].clip(upper=10)\n",
    "\n",
    "group_sum = df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*penalty).reset_index()\n",
    "group_sum.columns = ['chart_no', 'a', 'b', 'VA', 'CS', 'Cond','acuity']\n",
    "group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
    "group_sum.columns = ['a', 'b', 'VA', 'CS', 'Cond',model_name]\n",
    "group_sum\n",
    "# df_merged = pd.merge(df, group_sum, on='chart_no', how='left')\n",
    "# df_merged = df_merged[['image_id', 'file_name', 'Filter_no', 'text', 'rec_texts', 'chart_no',\n",
    "#                        'row_no', 'a', 'b', 'VA', 'CS', 'Cond', 'acuity']]\n",
    "# df_merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
