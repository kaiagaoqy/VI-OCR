{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fonts GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "dfs = []\n",
    "for phase in ['Phase1', 'Phase2']:\n",
    "    df = pd.read_excel(f'../../data/fonts/{phase}_Scoresheet.xlsx', )\n",
    "    df['a'] = df['Condition'].apply(lambda x: '1.00' if x == 'Normal' else '0.125')\n",
    "    df['b'] = '1.00'\n",
    "    df['Filter_no'] = df['Condition'].apply(lambda x: 1 if x == 'Normal' else 32)\n",
    "    df['group_id'] = df['Condition'].apply(lambda x: 0 if x == 'Normal' else 2)\n",
    "    # Create a group key\n",
    "    df['combo_count'] = df.groupby(['Font', 'Condition']).cumcount() + 1\n",
    "    df['group_id'] = df['group_id'] + 1 + ((df['combo_count']-1)  // 13).astype(int)\n",
    "    df['filename'] = f'{phase}_Img/' + phase +'_' + df['Font'] + '_' + df['Condition'] + '_' + df['group_id'].astype(str) + '_' + df['PrintSize'].round(1).astype(str) + '.png'\n",
    "    dfs.append(df.copy())\n",
    "pd.concat(dfs).to_csv('../../data/summary/fonts_compare.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def dataframe_to_coco(df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to COCO captioning format.\n",
    "    \n",
    "    Assumptions:\n",
    "      - The image path is constructed by concatenating the values from the \"a\" and \"row\" columns.\n",
    "      - The caption is built by concatenating the text from columns L1 to L15 (ignoring NaNs).\n",
    "    \"\"\"\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"info\": {\n",
    "            \"description\": \"MNRead Dataset\",\n",
    "            \"version\": \"1.0\"\n",
    "        },\n",
    "        \"licenses\": []\n",
    "    }\n",
    "    \n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "    \n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Construct the image file name. Here we assume column 'a' is the image number\n",
    "        # and 'row' is another identifier. Adjust formatting as needed.\n",
    "        file_name = row.get('filename')\n",
    "        caption = row.get('Sentence')\n",
    "        \n",
    "        # Add image entry (you may add width, height if known)\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": image_id if 'index' not in row else row['index'],\n",
    "            \"file_name\": file_name,\n",
    "            \"Filter_no\": row['Filter_no']\n",
    "        })\n",
    "        \n",
    "        # Add annotation entry for the caption\n",
    "        coco[\"annotations\"].append({\n",
    "            \"id\": annotation_id if 'index' not in row else row['index'],\n",
    "            \"image_id\": image_id if 'index' not in row else row['index'],\n",
    "            \"caption\": caption\n",
    "        })\n",
    "        \n",
    "        image_id += 1\n",
    "        annotation_id += 1\n",
    "        \n",
    "    return coco\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dict = dataframe_to_coco(pd.concat(dfs))\n",
    "    \n",
    "# Save the COCO JSON to a file\n",
    "with open(\"../../data/fonts/anno.json\", \"w\") as f:\n",
    "    json.dump(coco_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Chart Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "import os\n",
    "import inspect\n",
    "from copy import deepcopy\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt(file_path:str, word_num:int, \n",
    "           rename = {'Image.no':'image_no','print size':'print_size'},\n",
    "           L_R:bool = False, human=False) -> pd.DataFrame:\n",
    "    # Specify the path to your Excel file\n",
    "    \n",
    "    # Read the Excel file into a pandas DataFrame\n",
    "    xls = pd.ExcelFile(file_path)\n",
    "    # 获取所有sheet的名称\n",
    "    sheet_names = xls.sheet_names\n",
    "\n",
    "    # 逐个读取sheet，并存入列表\n",
    "    dfs = [pd.read_excel(xls, sheet_name=sheet) for sheet in sheet_names]\n",
    "\n",
    "    # 合并所有的DataFrame\n",
    "    gt = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # df = pd.read_excel(file_path, sheet_name=None, header=0)\n",
    "    \n",
    "    cols = ['SubID','print_size','image_no','a','b'] + ['L{}'.format(num) for num in range(1,word_num+1)]\n",
    "    # if human:\n",
    "    #     cols = ['SubID'] + cols\n",
    "    \n",
    "    if L_R:\n",
    "        cols += ['R{}'.format(num) for num in range(1,word_num+1)]\n",
    "\n",
    "    # gt = pd.concat(df,axis=0,ignore_index=True)\n",
    "    if rename is not None:\n",
    "        gt.rename(columns={'Image.no':'image_no','print size':'print_size'}, inplace=True)\n",
    "    gt['print_size'] = np.round(gt['print_size'],decimals=1)\n",
    "    gt['row'] = gt.groupby(['image_no','SubID']).apply(lambda x: np.round((x['print_size'].max()-x['print_size']+0.1)*10)).reset_index().set_index('level_2')['print_size']\n",
    "    \n",
    "    cols = ['row'] + cols\n",
    "    gt = gt.loc[:,cols].set_index(['print_size','image_no'])\n",
    "\n",
    "    \n",
    "    if human:\n",
    "        return gt\n",
    "\n",
    "\n",
    "    # 找到重复的索引\n",
    "    duplicate_index = gt.index.duplicated()\n",
    "\n",
    "    # 过滤出不重复的行\n",
    "    df_no_duplicates = gt[~duplicate_index]\n",
    "    return df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def dataframe_to_coco(df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to COCO captioning format.\n",
    "    \n",
    "    Assumptions:\n",
    "      - The image path is constructed by concatenating the values from the \"a\" and \"row\" columns.\n",
    "      - The caption is built by concatenating the text from columns L1 to L15 (ignoring NaNs).\n",
    "    \"\"\"\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"info\": {\n",
    "            \"description\": \"MNRead Dataset\",\n",
    "            \"version\": \"1.0\"\n",
    "        },\n",
    "        \"licenses\": []\n",
    "    }\n",
    "    \n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "    word_num = max([int(i[1:]) for i in df.columns if 'L' in i])\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Construct the image file name. Here we assume column 'a' is the image number\n",
    "        # and 'row' is another identifier. Adjust formatting as needed.\n",
    "        file_name = f\"{row['image_no']}_2260_{int(row['row'])}.jpg\"\n",
    "        \n",
    "        # Build caption by concatenating L1 to L15 (ignoring NaN values)\n",
    "        caption_parts = []\n",
    "        for i in range(1, word_num+1):\n",
    "            col_name = f\"L{i}\"\n",
    "            token = row.get(col_name)\n",
    "            if pd.notnull(token):\n",
    "                caption_parts.append(str(token))\n",
    "        caption = \" \".join(caption_parts)\n",
    "        \n",
    "        # Add image entry (you may add width, height if known)\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": image_id if 'index' not in row else row['index'],\n",
    "            \"file_name\": file_name,\n",
    "            \"Filter_no\": row['Filter_no']\n",
    "        })\n",
    "        \n",
    "        # Add annotation entry for the caption\n",
    "        coco[\"annotations\"].append({\n",
    "            \"id\": annotation_id if 'index' not in row else row['index'],\n",
    "            \"image_id\": image_id if 'index' not in row else row['index'],\n",
    "            \"caption\": caption\n",
    "        })\n",
    "        \n",
    "        image_id += 1\n",
    "        annotation_id += 1\n",
    "        \n",
    "    return coco\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2785062/999310692.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gt['row'] = gt.groupby(['image_no','SubID']).apply(lambda x: np.round((x['print_size'].max()-x['print_size']+0.1)*10)).reset_index().set_index('level_2')['print_size']\n",
      "/tmp/ipykernel_2785062/3183782216.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['a'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_2785062/3183782216.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['b'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_2785062/3183782216.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['a'].replace(0.287, 0.288, inplace=True)\n",
      "/tmp/ipykernel_2785062/3183782216.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['b'].replace(0.287, 0.288, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "filter_df = pd.read_csv('../../data/human/SelectedFilter.csv')\n",
    "\n",
    "xlsx_file_path = '../../data/human/MNREADChartScoreSheet.xlsx'\n",
    "WORD_NUM = 15\n",
    " \n",
    "df_no_duplicates = get_gt(file_path=xlsx_file_path, word_num=WORD_NUM)\n",
    "df_no_duplicates.drop(columns=['SubID'], inplace=True)\n",
    "df_no_duplicates['a'] = df_no_duplicates['a'].round(3)\n",
    "df_no_duplicates['b'] = df_no_duplicates['b'].round(3)\n",
    "df_no_duplicates['a'].replace(0.156, 0.157, inplace=True)\n",
    "df_no_duplicates['b'].replace(0.156, 0.157, inplace=True)\n",
    "df_no_duplicates['a'].replace(0.287, 0.288, inplace=True)\n",
    "df_no_duplicates['b'].replace(0.287, 0.288, inplace=True)\n",
    "df_no_duplicates.reset_index(inplace=True)\n",
    "df_no_duplicates = pd.merge(left=df_no_duplicates,right=filter_df[['a','b','Filter_no']],how='inner',on=['a','b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dict = dataframe_to_coco(df_no_duplicates)\n",
    "    \n",
    "# Save the COCO JSON to a file\n",
    "with open(\"../../data/mnread/anno.json\", \"w\") as f:\n",
    "    json.dump(coco_dict, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TotalText Anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def dataframe_to_coco_tt(df):\n",
    "    \"\"\"\n",
    "    Convert a DataFrame to COCO captioning format.\n",
    "    \n",
    "    Assumptions:\n",
    "      - The image path is constructed by concatenating the values from the \"a\" and \"row\" columns.\n",
    "      - The caption is built by concatenating the text from columns L1 to L15 (ignoring NaNs).\n",
    "    \"\"\"\n",
    "    coco = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"info\": {\n",
    "            \"description\": \"TotalText Dataset\",\n",
    "            \"version\": \"1.0\"\n",
    "        },\n",
    "        \"licenses\": []\n",
    "    }\n",
    "    \n",
    "    image_id = 0\n",
    "    annotation_id = 0\n",
    "\n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Construct the image file name. Here we assume column 'a' is the image number\n",
    "        # and 'row' is another identifier. Adjust formatting as needed.\n",
    "        file_name = f\"{row['image_no']}\"\n",
    "        \n",
    "        # Add image entry (you may add width, height if known)\n",
    "        coco[\"images\"].append({\n",
    "            \"id\": image_id if 'index' not in row else row['index'],\n",
    "            \"file_name\": file_name,\n",
    "            \"Filter_no\": row['Filter_no']\n",
    "        })\n",
    "        \n",
    "        # Add annotation entry for the caption\n",
    "        coco[\"annotations\"].append({\n",
    "            \"id\": annotation_id if 'index' not in row else row['index'],\n",
    "            \"image_id\": image_id if 'index' not in row else row['index'],\n",
    "            \"caption\": row['caption'],\n",
    "            \"bbox\": row['bbox'],\n",
    "            \"ornt\": row['ornt'] if 'ornt' in row else []\n",
    "        })\n",
    "        \n",
    "        image_id += 1\n",
    "        annotation_id += 1\n",
    "        \n",
    "    return coco\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "gts = glob.glob('../../data/viocr/totaltext/gt/*.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builtins import enumerate, list, open\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "files = glob.glob(\"/cis/home/qgao14/my_documents/VIOCR_infer_models/data/viocr/totaltext/txt_format/Test/*.txt\")\n",
    "df = {'image_no':[],'Filter_no':[],'a':[],'b':[],'label':[],'points':[],'ornt':[]}\n",
    "filters=[1,2,3,4,5,6,7,32,33,34,35,36,38,39,40,41]\n",
    "HShiftList = [1.000, 0.288, 0.157, 0.086, 0.048, 0.027,\n",
    "    0.250, 0.134, 0.072, 0.039, 0.022,\n",
    "    0.267, 0.144, 0.078, 0.043, 0.024,\n",
    "    0.314, 0.172, 0.096, 0.055, 0.032,\n",
    "    0.345, 0.193, 0.110, 0.064, 0.038,\n",
    "    0.439, 0.256, 0.154, 0.033, 0.018,\n",
    "    0.125, 0.063, 0.031, 0.016, 1.000,\n",
    "    1.000, 1.000, 1.000, 1.000, 1.000]\n",
    "\n",
    "VShiftList = [1.000, 0.288, 0.157, 0.086, 0.048, 0.027,\n",
    "    1.000, 0.534, 0.288, 0.157, 0.086,\n",
    "    0.534, 0.288, 0.157, 0.086, 0.048,\n",
    "    0.157, 0.086, 0.048, 0.027, 0.016,\n",
    "    0.086, 0.048, 0.027, 0.016, 0.010,\n",
    "    0.027, 0.016, 0.010, 0.534, 0.288,\n",
    "    1.000, 1.000, 1.000, 1.000, 0.355,\n",
    "    0.178, 0.089, 0.045, 0.022, 0.011]\n",
    "for filter_no in filters:\n",
    "    for id, file in enumerate(files):\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 删除换行符，避免数组分行导致正则失败\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "        # 匹配每一条完整的记录\n",
    "        pattern = re.compile(\n",
    "            r\"x:\\s*\\[\\[([-\\d\\s]+)\\]\\],\\s*\"\n",
    "            r\"y:\\s*\\[\\[([-\\d\\s]+)\\]\\],\\s*\"\n",
    "            r\"ornt:\\s*\\[u'(.+?)'\\],\\s*\"\n",
    "            r\"transcriptions:\\s*\\[u'(.+?)'\\]\"\n",
    "        )\n",
    "\n",
    "        data = []\n",
    "        for x_str, y_str, ornt, transcription in pattern.findall(text):\n",
    "            x = list(map(int, x_str.split()))\n",
    "            y = list(map(int, y_str.split()))\n",
    "            data.append({\n",
    "                \"x\": x,\n",
    "                \"y\": y,\n",
    "                \"ornt\": ornt,\n",
    "                \"transcription\": transcription\n",
    "            })\n",
    "        for item in data:\n",
    "            try:\n",
    "                points = list(zip(item['x'], item['y']))\n",
    "                points = [[list(p) for p in points]]\n",
    "            \n",
    "                #df['image_no'].append(f\"{id:07d}.jpg\")\n",
    "                df['image_no'].append(f\"{os.path.basename(file).split('_')[-1].replace('.txt','')}.jpg\")\n",
    "                df['Filter_no'].append(filter_no)\n",
    "                df['a'].append(HShiftList[filter_no-1])\n",
    "                df['b'].append(VShiftList[filter_no-1])\n",
    "                df['label'].append(item['transcription'])\n",
    "                df['points'].append(points)\n",
    "                df['ornt'].append(item['ornt'])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing line in file {file}: {item}. Error: {e}\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = {'image_no':[],'Filter_no':[],'a':[],'b':[],'label':[],'points':[]}\n",
    "# filters=[1,2,3,4,5,6,7,32,33,34,35,36,38,39,40,41]\n",
    "# HShiftList = [1.000, 0.288, 0.157, 0.086, 0.048, 0.027,\n",
    "#     0.250, 0.134, 0.072, 0.039, 0.022,\n",
    "#     0.267, 0.144, 0.078, 0.043, 0.024,\n",
    "#     0.314, 0.172, 0.096, 0.055, 0.032,\n",
    "#     0.345, 0.193, 0.110, 0.064, 0.038,\n",
    "#     0.439, 0.256, 0.154, 0.033, 0.018,\n",
    "#     0.125, 0.063, 0.031, 0.016, 1.000,\n",
    "#     1.000, 1.000, 1.000, 1.000, 1.000]\n",
    "\n",
    "# VShiftList = [1.000, 0.288, 0.157, 0.086, 0.048, 0.027,\n",
    "#     1.000, 0.534, 0.288, 0.157, 0.086,\n",
    "#     0.534, 0.288, 0.157, 0.086, 0.048,\n",
    "#     0.157, 0.086, 0.048, 0.027, 0.016,\n",
    "#     0.086, 0.048, 0.027, 0.016, 0.010,\n",
    "#     0.027, 0.016, 0.010, 0.534, 0.288,\n",
    "#     1.000, 1.000, 1.000, 1.000, 0.355,\n",
    "#     0.178, 0.089, 0.045, 0.022, 0.011]\n",
    "# # with open('../../data/totaltext/gt/0000002.json','r') as f:\n",
    "# #     gt = json.load(f)\n",
    "# for filter_no in filters:\n",
    "#     for gt_path in gts:\n",
    "#         with open(gt_path,'r') as f:\n",
    "#             gt = json.load(f)\n",
    "#         for shape in gt['shapes']:   \n",
    "#             df['image_no'].append(gt['imagePath'].split('/')[-1])\n",
    "#             df['Filter_no'].append(filter_no)\n",
    "#             df['a'].append(HShiftList[filter_no-1])\n",
    "#             df['b'].append(VShiftList[filter_no-1])\n",
    "#             df['label'].append(shape['label'])\n",
    "#             df['points'].append(shape['points'])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4088786/2809971420.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['image_no','Filter_no','a','b']).apply(lambda x: [' '.join(x['label']), [i for i in x['points']], [' '.join(x['ornt'])]]).apply(pd.Series).reset_index()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(df)\n",
    "df = df.groupby(['image_no','Filter_no','a','b']).apply(lambda x: [' '.join(x['label']), [i for i in x['points']], [' '.join(x['ornt'])]]).apply(pd.Series).reset_index()\n",
    "df.rename(columns={0:'caption',1:'bbox',2:'ornt'},inplace=True)\n",
    "filter_df = pd.read_csv('../../data/human/SelectedFilter.csv')\n",
    "df = pd.merge(left=df,right=filter_df,how='inner',on=['a','b'])\n",
    "df.rename(columns={'Filter_no_x':'Filter_no_old','Filter_no_y':'Filter_no'},inplace=True)\n",
    "df['image_no'] = df.apply(lambda x: f\"{x['Filter_no']}/{x['image_no']}\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "coco_dict = dataframe_to_coco_tt(df)\n",
    "    \n",
    "# Save the COCO JSON to a file\n",
    "# with open(\"../../data/totaltext/anno.json\", \"w\") as f:\n",
    "with open(\"../../data/totaltext_all/anno.json\", \"w\") as f:\n",
    "    json.dump(coco_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([16, 10, 8, 6, 4, 2, 9, 7, 5, 3, 1, 15, 14, 13, 12, 11])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{1: 16, 2: 10, 3: 8, 4: 6, 5: 4, 6: 2, 7: 9, \n",
    "                        32: 7, 33: 5, 34: 3, 35: 1, 36: 15, 38: 14,\n",
    "                        39: 13, 40: 12, 41: 11}.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xlsx_file_path = '../../data/human/LetterChartScoreSheet.xlsx'\n",
    "# WORD_NUM = 5\n",
    " \n",
    "# df_no_duplicates = get_gt(file_path=xlsx_file_path, word_num=WORD_NUM)\n",
    "# df_no_duplicates.drop(columns=['SubID'], inplace=True)\n",
    "# df_no_duplicates['a'] = df_no_duplicates['a'].round(3)\n",
    "# df_no_duplicates['b'] = df_no_duplicates['b'].round(3)\n",
    "# df_no_duplicates['a'].replace(0.156, 0.157, inplace=True)\n",
    "# df_no_duplicates['b'].replace(0.156, 0.157, inplace=True)\n",
    "# df_no_duplicates['a'].replace(0.287, 0.288, inplace=True)\n",
    "# df_no_duplicates['b'].replace(0.287, 0.288, inplace=True)\n",
    "# df_no_duplicates.reset_index(inplace=True)\n",
    "# df_no_duplicates = pd.merge(left=df_no_duplicates,right=filter_df[['a','b','Filter_no']],how='inner',on=['a','b'])\n",
    "# coco_dict = dataframe_to_coco(df_no_duplicates)\n",
    "    \n",
    "# # Save the COCO JSON to a file\n",
    "# with open(\"../../data/etdrs/anno.json\", \"w\") as f:\n",
    "#     json.dump(coco_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update ETDRS GT\n",
    "- the forth line of every chart is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_rows(group):\n",
    "    \"\"\"\n",
    "    For each group (chart), if the maximum 'row' equals 21,\n",
    "    then starting from the row where 'row' is >= 4 and for each row (except the last),\n",
    "    replace its L1-L5 values with the values from the next row.\n",
    "    \"\"\"\n",
    "    # Sort by the 'row' column (if not already sorted)\n",
    "    group = group.sort_values(by='row').reset_index(drop=True)\n",
    "    # Check if last row (i.e. max row number) is 21.\n",
    "    if group['row'].max() == 21:\n",
    "        # Iterate over rows from the beginning to the second last row.\n",
    "        for i in range(len(group) - 1):\n",
    "            # Check if the current row's 'row' value is >= 4.\n",
    "            if group.loc[i, 'row'] >= 4:\n",
    "                # Replace L1 to L5 with the values from the next row.\n",
    "                group.loc[i, ['L1', 'L2', 'L3', 'L4', 'L5']] = group.loc[i+1, ['L1', 'L2', 'L3', 'L4', 'L5']]\n",
    "    if group.loc[0, ['L1', 'L2', 'L3', 'L4', 'L5']].isna().any():\n",
    "        for i in range(len(group) - 2):\n",
    "            # If any of L1-L5 is NaN in the current row\n",
    "            if group.loc[i, ['L1', 'L2', 'L3', 'L4', 'L5']].isna().any():\n",
    "                # Replace this row's L1-L5 with the next row's values\n",
    "                group.loc[i, ['L1', 'L2', 'L3', 'L4', 'L5']] = group.loc[i+2, ['L1', 'L2', 'L3', 'L4', 'L5']]\n",
    "                group.loc[i+2, ['L1', 'L2', 'L3', 'L4', 'L5']] = np.nan\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1911478/999310692.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gt['row'] = gt.groupby(['image_no','SubID']).apply(lambda x: np.round((x['print_size'].max()-x['print_size']+0.1)*10)).reset_index().set_index('level_2')['print_size']\n",
      "/tmp/ipykernel_1911478/3878845389.py:9: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['a'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_1911478/3878845389.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['b'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_1911478/3878845389.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['a'].replace(0.287, 0.288, inplace=True)\n",
      "/tmp/ipykernel_1911478/3878845389.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_no_duplicates['b'].replace(0.287, 0.288, inplace=True)\n",
      "/tmp/ipykernel_1911478/3878845389.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_no_duplicates = df_no_duplicates.groupby('image_no').apply(replace_rows).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "filter_df = pd.read_csv('../../data/human/SelectedFilter.csv')\n",
    "xlsx_file_path = '../../data/human/LetterChartScoreSheet.xlsx'\n",
    "WORD_NUM = 5\n",
    " \n",
    "df_no_duplicates = get_gt(file_path=xlsx_file_path, word_num=WORD_NUM)\n",
    "df_no_duplicates.drop(columns=['SubID'], inplace=True)\n",
    "df_no_duplicates['a'] = df_no_duplicates['a'].round(3)\n",
    "df_no_duplicates['b'] = df_no_duplicates['b'].round(3)\n",
    "df_no_duplicates['a'].replace(0.156, 0.157, inplace=True)\n",
    "df_no_duplicates['b'].replace(0.156, 0.157, inplace=True)\n",
    "df_no_duplicates['a'].replace(0.287, 0.288, inplace=True)\n",
    "df_no_duplicates['b'].replace(0.287, 0.288, inplace=True)\n",
    "df_no_duplicates.reset_index(inplace=True)\n",
    "df_no_duplicates = pd.merge(left=df_no_duplicates,right=filter_df[['a','b','Filter_no']],how='inner',on=['a','b'])\n",
    "# Group by 'image_no' (each chart) and apply the replacement function.\n",
    "df_no_duplicates = df_no_duplicates.reset_index()\n",
    "df_no_duplicates = df_no_duplicates.groupby('image_no').apply(replace_rows).reset_index(drop=True)\n",
    "df_no_duplicates = df_no_duplicates[df_no_duplicates['row'] != 21]\n",
    "df_no_duplicates = df_no_duplicates.set_index('index').sort_index()\n",
    "# coco_dict = dataframe_to_coco(df_no_duplicates.dropna().reset_index())\n",
    "    \n",
    "# Save the COCO JSON to a file\n",
    "# with open(\"../../data/etdrs/anno.json\", \"w\") as f:\n",
    "#     json.dump(coco_dict, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SeeingAI Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file_path = '../../data/summary/SeeingAI.xlsx'\n",
    "xls = pd.ExcelFile(excel_file_path)\n",
    "sheet_names = xls.sheet_names\n",
    "dfs = [pd.read_excel(xls, sheet_name=sheet) for sheet in sheet_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_matches_per_row(row):\n",
    "    left_side = pd.Series(row[['L1', 'L2', 'L3', 'L4', 'L5']]).str.lower()\n",
    "    right_side = pd.Series(row[['R1', 'R2', 'R3', 'R4', 'R5']]).astype(str).str.lower()\n",
    "    \n",
    "    counter_left = Counter(left_side.dropna()) # dic\n",
    "    counter_right = Counter(right_side.dropna()) if right_side.dropna().any() else Counter('')\n",
    "    \n",
    "    # 计算两侧相同元素的个数，确保每个元素只计算一次\n",
    "    matches = sum((counter_left & counter_right).values())\n",
    "    total = sum(counter_left.values())\n",
    "    \n",
    "    # char_level\n",
    "    counter_left_char = Counter(''.join(left_side.dropna().tolist()))\n",
    "    counter_right_char = Counter(''.join(right_side.dropna().tolist())) if right_side.dropna().any() else Counter('')\n",
    "    matches_char = sum((counter_left_char & counter_right_char).values())\n",
    "    total_char = sum(counter_left_char.values())\n",
    "    return [matches, total - matches, total_char-matches_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df = pd.read_csv('../../data/human/SelectedFilter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1911478/3933131642.py:2: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  etdre_df['image_no'] = etdre_df['image_no'].fillna(method='ffill')\n",
      "/tmp/ipykernel_1911478/3933131642.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = etdre_df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*0.02).reset_index()\n",
      "/tmp/ipykernel_1911478/3933131642.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n"
     ]
    }
   ],
   "source": [
    "etdre_df = dfs[0]\n",
    "etdre_df['image_no'] = etdre_df['image_no'].fillna(method='ffill')\n",
    "etdre_df.drop(columns=['Unnamed: 7'], inplace=True)\n",
    "etdre_df = pd.merge(etdre_df,df_no_duplicates.dropna().reset_index().drop(columns=['L1','L2','L3','L4', 'L5']),how='left',on=['image_no','print_size'])\n",
    "etdre_df[['match','missing','missing_char']] = etdre_df.apply(lambda row: count_matches_per_row(row), axis=1).apply(pd.Series)\n",
    "etdre_df['missing_clipped'] = etdre_df['missing'].clip(upper=5)\n",
    "etdre_df = pd.merge(etdre_df, filter_df, how='left', on=['a','b','Filter_no'])\n",
    "etdre_df.rename(columns={'image_no':'chart_no'}, inplace=True)\n",
    "group_sum = etdre_df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.2 + x['missing_clipped'].sum()*0.02).reset_index()\n",
    "group_sum.columns = ['chart_no', 'a', 'b', 'VA', 'CS', 'Cond','acuity']\n",
    "group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
    "group_sum.columns = ['a', 'b', 'VA', 'CS', 'Cond','SeeingAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/cis/home/qgao14/my_documents/VIOCR_infer_models/data/etdrs/anno.json','r') as f:\n",
    "    anno = json.load(f)\n",
    "anno = {anno['images'][i]['file_name']: anno['images'][i]['id'] for i in range(len(anno['images']))}\n",
    "etdre_df['image_no'] = etdre_df['chart_no'] + '_2260_' + (etdre_df.groupby('chart_no').cumcount() + 1).astype(str) + '.jpg'\n",
    "etdre_df['image_no'] = etdre_df['image_no'].apply(lambda x: anno.get(x, None))\n",
    "\n",
    "out = []\n",
    "for id, row in etdre_df.dropna(subset=['image_no']).iterrows():\n",
    "    if row['image_no'] is not None:\n",
    "        out.append({\n",
    "            \"image_id\": int(row['image_no']),\n",
    "            \"category_id\": 1,\n",
    "            \"polys\": [],\n",
    "            \"rec_texts\": row[['R1','R2','R3','R4','R5']].dropna().astype(str).tolist(),\n",
    "            \"rec_score\": 0,\n",
    "            \"det_score\": 0,\n",
    "            \"filter\": int(row['Filter_no'])\n",
    "        })\n",
    "with open('/cis/home/qgao14/my_documents/VIOCR_infer_models/filtered/etdrs/seeingai.json','w') as f:\n",
    "    json.dump(out, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('../../data/summary/etdrs_combined_1021.csv')\n",
    "pd.merge(final, group_sum, how='left', on=['a','b','VA','CS','Cond']).to_csv('../../data/summary/etdrs_combined_1021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_matches_per_row(row):\n",
    "    left_side = pd.Series(row[['L{}'.format(i) for i in range(1,16)]]).astype(str).str.lower()\n",
    "    right_side = pd.Series(row[['R{}'.format(i) for i in range(1,16)]]).astype(str).str.lower()\n",
    "    \n",
    "    counter_left = Counter(left_side.dropna()) # dic\n",
    "    counter_right = Counter(right_side.dropna()) if right_side.dropna().any() else Counter('')\n",
    "    \n",
    "    # 计算两侧相同元素的个数，确保每个元素只计算一次\n",
    "    matches = sum((counter_left & counter_right).values())\n",
    "    total = sum(counter_left.values())\n",
    "    \n",
    "    # char_level\n",
    "    counter_left_char = Counter(''.join(left_side.dropna().tolist()))\n",
    "    counter_right_char = Counter(''.join(right_side.dropna().tolist())) if right_side.dropna().any() else Counter('')\n",
    "    matches_char = sum((counter_left_char & counter_right_char).values())\n",
    "    total_char = sum(counter_left_char.values())\n",
    "    return [matches, total - matches, total_char-matches_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1911478/1819217393.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = mnread_df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.4 + x['missing_clipped'].sum()*0.01).reset_index()\n",
      "/tmp/ipykernel_1911478/1819217393.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n"
     ]
    }
   ],
   "source": [
    "mnread_df = dfs[1].copy()\n",
    "mnread_df.drop(columns=['a','b'], inplace=True)\n",
    "mnread_df.rename(columns={'Image.no':'image_no','print size':'print_size'}, inplace=True)\n",
    "mnread_df = pd.merge(mnread_df,df_no_duplicates[['print_size', 'image_no', 'row', 'a', 'b','Filter_no']],how='left',on=['image_no','print_size'])\n",
    "mnread_df[['match','missing','missing_char']] = mnread_df.apply(lambda row: count_matches_per_row(row), axis=1).apply(pd.Series)\n",
    "mnread_df['missing_clipped'] = mnread_df['missing'].clip(upper=10)\n",
    "mnread_df = pd.merge(mnread_df, filter_df, how='left', on=['a','b','Filter_no'])\n",
    "mnread_df.rename(columns={'image_no':'chart_no'}, inplace=True)\n",
    "mnread_df['print_size'] -=0.8\n",
    "mnread_df = mnread_df.loc[mnread_df['print_size'] >= -0.4, :]\n",
    "group_sum = mnread_df.groupby(['chart_no','a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: -0.4 + x['missing_clipped'].sum()*0.01).reset_index()\n",
    "group_sum.columns = ['chart_no', 'a', 'b', 'VA', 'CS', 'Cond','acuity']\n",
    "group_sum = group_sum.groupby(['a', 'b', 'VA', 'CS', 'Cond']).apply(lambda x: x['acuity'].mean()).reset_index()\n",
    "group_sum.columns = ['a', 'b', 'VA', 'CS', 'Cond','SeeingAI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.read_csv('../../data/summary/mnread_combined_1021.csv')\n",
    "pd.merge(final, group_sum, how='left', on=['a','b','VA','CS','Cond']).to_csv('../../data/summary/mnread_combined_1021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/cis/home/qgao14/my_documents/VIOCR_infer_models/data/mnread/anno.json','r') as f:\n",
    "    anno = json.load(f)\n",
    "anno = {anno['images'][i]['file_name']: anno['images'][i]['id'] for i in range(len(anno['images']))}\n",
    "mnread_df['image_no'] = mnread_df['chart_no'] + '_1680_' + (mnread_df.groupby('chart_no').cumcount() + 1).astype(str) + '.png'\n",
    "mnread_df['image_no'] = mnread_df['image_no'].apply(lambda x: anno.get(x, None))\n",
    "\n",
    "out = []\n",
    "for id, row in mnread_df.dropna(subset=['Filter_no']).iterrows():\n",
    "    if row['image_no'] is not None:\n",
    "        out.append({\n",
    "            \"image_id\": int(row['image_no']),\n",
    "            \"category_id\": 1,\n",
    "            \"polys\": [],\n",
    "            \"rec_texts\": row[['R1','R2','R3','R4','R5','R6','R7','R8','R9','R10','R11','R12','R13','R14','R15']].dropna().astype(str).tolist(),\n",
    "            \"rec_score\": 0,\n",
    "            \"det_score\": 0,\n",
    "            \"filter\": int(row['Filter_no'])\n",
    "        })\n",
    "with open('/cis/home/qgao14/my_documents/VIOCR_infer_models/filtered/mnread/seeingai.json','w') as f:\n",
    "    json.dump(out, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Acuity on differnt filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_matches_per_row(row):\n",
    "    left = ['L'+str(i+1) for i in range(WORD_NUM)]\n",
    "    left_side = row[left].str.lower()\n",
    "    right = ['R'+str(i+1) for i in range(WORD_NUM)]\n",
    "    right_side = row[right].str.lower()\n",
    "    \n",
    "    counter_left = Counter(left_side.dropna()) # dic\n",
    "    counter_right = Counter(right_side.dropna())\n",
    "    \n",
    "    # 计算两侧相同元素的个数，确保每个元素只计算一次\n",
    "    matches = sum((counter_left & counter_right).values())\n",
    "    total = sum(counter_left.values())\n",
    "    \n",
    "    # char_level\n",
    "    counter_left_char = Counter(''.join(left_side.dropna().tolist()))\n",
    "    counter_right_char = Counter(''.join(right_side.dropna().tolist()))\n",
    "    matches_char = sum((counter_left_char & counter_right_char).values())\n",
    "    total_char = sum(counter_left_char.values())\n",
    "    return [matches, total - matches, total_char-matches_char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2968555/999310692.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gt['row'] = gt.groupby(['image_no','SubID']).apply(lambda x: np.round((x['print_size'].max()-x['print_size']+0.1)*10)).reset_index().set_index('level_2')['print_size']\n",
      "/tmp/ipykernel_2968555/154598817.py:9: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_human = df_human.groupby(['image_no','a','b','SubID']).apply(lambda x: -0.3 + x['missing_clipped'].sum()*0.01).reset_index()\n",
      "/tmp/ipykernel_2968555/154598817.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_human['a'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_2968555/154598817.py:14: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_human['b'].replace(0.156, 0.157, inplace=True)\n",
      "/tmp/ipykernel_2968555/154598817.py:15: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_human['a'].replace(0.287, 0.288, inplace=True)\n",
      "/tmp/ipykernel_2968555/154598817.py:16: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_human['b'].replace(0.287, 0.288, inplace=True)\n",
      "/tmp/ipykernel_2968555/154598817.py:17: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_human = df_human.groupby(['image_no','a','b']).apply(lambda x: x['acuity'].mean())\n",
      "/tmp/ipykernel_2968555/154598817.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_human = df_human.groupby(['a','b']).apply(lambda x: x['acuity'].mean())\n"
     ]
    }
   ],
   "source": [
    "xlsx_file_path = '../../data/human/MNREADChartScoreSheet.xlsx'\n",
    "WORD_NUM = 15\n",
    "df_human = get_gt(file_path=xlsx_file_path, word_num=WORD_NUM,human=True,L_R=True)\n",
    "df_human[['match','missing','missing_char']] = df_human.apply(count_matches_per_row, axis=1, result_type='expand')\n",
    "df_human.reset_index(inplace=True)\n",
    "df_human['missing_clipped'] = df_human['missing'].clip(upper=10)\n",
    "# df_human = df_human[df_human['print_size'] > -.3]\n",
    "# df_human = df_human[df_human['SubID'] != 'N1']\n",
    "df_human = df_human.groupby(['image_no','a','b','SubID']).apply(lambda x: -0.3 + x['missing_clipped'].sum()*0.01).reset_index()\n",
    "df_human.rename(columns={0:'acuity'}, inplace=True)\n",
    "df_human['a'] = df_human['a'].round(3)\n",
    "df_human['b'] = df_human['b'].round(3)\n",
    "df_human['a'].replace(0.156, 0.157, inplace=True)\n",
    "df_human['b'].replace(0.156, 0.157, inplace=True)\n",
    "df_human['a'].replace(0.287, 0.288, inplace=True)\n",
    "df_human['b'].replace(0.287, 0.288, inplace=True)\n",
    "df_human = df_human.groupby(['image_no','a','b']).apply(lambda x: x['acuity'].mean())\n",
    "df_human = df_human.reset_index()\n",
    "df_human.rename(columns={0:'acuity'}, inplace=True)\n",
    "df_human = df_human.groupby(['a','b']).apply(lambda x: x['acuity'].mean())\n",
    "df_human = df_human.reset_index().rename(columns={0:'acuity'})\n",
    "df_human.to_csv('../../data/human/human_mnread_acuity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_761550/999310692.py:30: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  gt['row'] = gt.groupby(['image_no','SubID']).apply(lambda x: np.round((x['print_size'].max()-x['print_size']+0.1)*10)).reset_index().set_index('level_2')['print_size']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'count_matches_per_row' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m WORD_NUM \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m      3\u001b[0m df_human \u001b[38;5;241m=\u001b[39m get_gt(file_path\u001b[38;5;241m=\u001b[39mxlsx_file_path, word_num\u001b[38;5;241m=\u001b[39mWORD_NUM,human\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,L_R\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m df_human[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmissing_char\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m df_human\u001b[38;5;241m.\u001b[39mapply(\u001b[43mcount_matches_per_row\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexpand\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m df_human\u001b[38;5;241m.\u001b[39mreset_index(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df_human = df_human[df_human['print_size'] > -.3]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# df_human = df_human[df_human['SubID'] != 'N1']\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'count_matches_per_row' is not defined"
     ]
    }
   ],
   "source": [
    "xlsx_file_path = '../../data/human/LetterChartScoreSheet.xlsx'\n",
    "WORD_NUM = 5\n",
    "df_human = get_gt(file_path=xlsx_file_path, word_num=WORD_NUM,human=True,L_R=True)\n",
    "df_human[['match','missing','missing_char']] = df_human.apply(count_matches_per_row, axis=1, result_type='expand')\n",
    "df_human.reset_index(inplace=True)\n",
    "# df_human = df_human[df_human['print_size'] > -.3]\n",
    "# df_human = df_human[df_human['SubID'] != 'N1']\n",
    "df_human['missing_clipped'] = df_human['missing'].clip(upper=WORD_NUM)\n",
    "df_human = df_human.groupby(['image_no','a','b','SubID']).apply(lambda x: -0.3 + x['missing_clipped'].sum()*0.02).reset_index()\n",
    "df_human.rename(columns={0:'acuity'}, inplace=True)\n",
    "df_human['a'] = df_human['a'].round(3)\n",
    "df_human['b'] = df_human['b'].round(3)\n",
    "df_human['a'].replace(0.156, 0.157, inplace=True)\n",
    "df_human['b'].replace(0.156, 0.157, inplace=True)\n",
    "df_human['a'].replace(0.287, 0.288, inplace=True)\n",
    "df_human['b'].replace(0.287, 0.288, inplace=True)\n",
    "df_human = df_human.groupby(['image_no','a','b']).apply(lambda x: x['acuity'].mean())\n",
    "df_human = df_human.reset_index()\n",
    "df_human.rename(columns={0:'acuity'}, inplace=True)\n",
    "df_human = df_human.groupby(['a','b']).apply(lambda x: x['acuity'].mean())\n",
    "df_human = df_human.reset_index().rename(columns={0:'acuity'})\n",
    "df_human.to_csv('../../data/human/human_etdrs_acuity.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Scene Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "excel_file_path = '../../data/human/subject_result_v5.xlsx'\n",
    "xls = pd.ExcelFile(excel_file_path)\n",
    "sheet_names = xls.sheet_names\n",
    "dfs = {sheet:pd.read_excel(xls, sheet_name=sheet) for sheet in sheet_names if 'rec' in sheet}\n",
    "for name, sheet in dfs.items():\n",
    "    sheet['SubID'] = name.split('_')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_56763/971761419.py:2: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['SubID','a','b']).apply(lambda x: x['hmean'].mean()).reset_index()\n",
      "/tmp/ipykernel_56763/971761419.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby(['a','b']).apply(lambda x: [x['hmean'].mean(),x['hmean'].std()]).apply(pd.Series).reset_index()\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(dfs.values(),axis=0)\n",
    "df = df.groupby(['SubID','a','b']).apply(lambda x: x['hmean'].mean()).reset_index()\n",
    "df.rename(columns={0:'hmean'}, inplace=True)\n",
    "df = df.groupby(['a','b']).apply(lambda x: [x['hmean'].mean(),x['hmean'].std()]).apply(pd.Series).reset_index()\n",
    "df.rename(columns={0:'human',1:'human_err'}, inplace=True)\n",
    "df.to_csv('../../data/human/human_totaltext_acuity.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
